{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe79d9b-9091-40e1-b3b5-eb277c7ef821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>909</td><td>application_1732639283265_0876</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_0876/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_0876_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7426ad21-8a64-447c-94e4-0466b11fa5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1882</td><td>application_1732639283265_1843</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1843/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1843_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '4', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1867</td><td>application_1732639283265_1828</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1828/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1828_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1868</td><td>application_1732639283265_1829</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1829/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1829_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1870</td><td>application_1732639283265_1831</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1831/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1831_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1871</td><td>application_1732639283265_1832</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1832/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1832_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1875</td><td>application_1732639283265_1836</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1836/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1836_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1877</td><td>application_1732639283265_1838</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1838/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1838_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1878</td><td>application_1732639283265_1839</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1839/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1839_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1880</td><td>application_1732639283265_1841</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1841/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1841_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1881</td><td>application_1732639283265_1842</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1842/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1842_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1882</td><td>application_1732639283265_1843</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1843/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1843_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d9f489d-c8b3-4e6e-a93a-1944bcbda2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DR_NO: string (nullable = true)\n",
      " |-- Date Rptd: string (nullable = true)\n",
      " |-- DATE OCC: string (nullable = true)\n",
      " |-- TIME OCC: string (nullable = true)\n",
      " |-- AREA: integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: float (nullable = true)\n",
      " |-- LON: float (nullable = true)\n",
      "\n",
      "Number of rows in the children DataFrame: 15928\n",
      "Number of rows in the young_adults DataFrame: 33605\n",
      "Number of rows in the adults DataFrame: 121093\n",
      "Number of rows in the oldies DataFrame: 5985\n",
      "Time taken: 28.20 seconds"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col,desc\n",
    "import time\n",
    "\n",
    "# Initialize spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date Rptd\", StringType(), True),\n",
    "    StructField(\"DATE OCC\", StringType(), True),\n",
    "    StructField(\"TIME OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", IntegerType(), True),\n",
    "    StructField(\"AREA NAME\", StringType(), True),\n",
    "    StructField(\"Rpt Dist No\", IntegerType(), True),\n",
    "    StructField(\"Part 1-2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict Age\", IntegerType(), True),\n",
    "    StructField(\"Vict Sex\", StringType(), True),\n",
    "    StructField(\"Vict Descent\", StringType(), True),\n",
    "    StructField(\"Premis Cd\", IntegerType(), True),\n",
    "    StructField(\"Premis Desc\", StringType(), True),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType(), True),\n",
    "    StructField(\"Weapon Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status Desc\", StringType(), True),\n",
    "    StructField(\"Crm Cd 1\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 2\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 3\", IntegerType(), True),\n",
    "    StructField(\"Crm Cd 4\", IntegerType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross Street\", StringType(), True),\n",
    "    StructField(\"LAT\", FloatType(), True),\n",
    "    StructField(\"LON\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Starting Time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the first CSV (2010-2019) into a DataFrame\n",
    "df_1 = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "# Load the second CSV (2020-present) into a DataFrame\n",
    "df_2 = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "# Union both DataFrames into a single DataFrame\n",
    "df = df_1.union(df_2)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "filtered_df = df.filter(col(\"Crm Cd Desc\").rlike(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "children_df = filtered_df.filter(col(\"Vict Age\") < 18)\n",
    "young_adults_df = filtered_df.filter((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24))\n",
    "adults_df = filtered_df.filter((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64))\n",
    "oldies_df = filtered_df.filter(col(\"Vict Age\") > 64)\n",
    "\n",
    "children_df.orderBy(desc(\"Vict Age\")).select(\"Vict Age\")\n",
    "young_adults_df.orderBy(desc(\"Vict Age\")).select(\"Vict Age\")\n",
    "adults_df.orderBy(desc(\"Vict Age\")).select(\"Vict Age\")\n",
    "oldies_df.orderBy(desc(\"Vict Age\")).select(\"Vict Age\")\n",
    "# Count the number of rows in the DataFrame\n",
    "print(f\"Number of rows in the children DataFrame: {children_df.count()}\")\n",
    "print(f\"Number of rows in the young_adults DataFrame: {young_adults_df.count()}\")\n",
    "print(f\"Number of rows in the adults DataFrame: {adults_df.count()}\")\n",
    "print(f\"Number of rows in the oldies DataFrame: {oldies_df.count()}\")\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c182482b-6a2d-4c08-a537-80fc0cb888bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the children RDD: 15928\n",
      "Number of rows in the young_adults RDD: 33605\n",
      "Number of rows in the adults RDD: 121093\n",
      "Number of rows in the oldies RDD: 5985\n",
      "Time taken: 69.66 seconds"
     ]
    }
   ],
   "source": [
    "# Spark RDD code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark import RDD\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize Spark session\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "# Starting time\n",
    "start_time = time.time()\n",
    "\n",
    "def parse_csv(line):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "rdd_1 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\") \\\n",
    "    .map(parse_csv)\n",
    "\n",
    "\n",
    "rdd_2 = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\") \\\n",
    "    .map(parse_csv)\n",
    "\n",
    "# Union both RDDs\n",
    "rdd = rdd_1.union(rdd_2)\n",
    "\n",
    "# Filter the initial data\n",
    "filtered_rdd = rdd.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[9])  # \"Crm Cd Desc\" is at index 9\n",
    "\n",
    "# Split the filtered data into different age groups\n",
    "children_rdd = filtered_rdd.filter(lambda row: int(row[11]) < 18)  # \"Vict Age\" is at index 11\n",
    "young_adults_rdd = filtered_rdd.filter(lambda row: 18 <= int(row[11]) <= 24)\n",
    "adults_rdd = filtered_rdd.filter(lambda row: 25 <= int(row[11]) <= 64)\n",
    "oldies_rdd = filtered_rdd.filter(lambda row: int(row[11]) > 64)\n",
    "\n",
    "# select the age as key and sort by key in descending orde\n",
    "children_sorted = children_rdd.map(lambda x: [int(x[11]), x[0]]) \\\n",
    "                    .sortByKey(ascending=False)\n",
    "young_adults_sorted = young_adults_rdd.map(lambda x: [int(x[11]), x[0]]) \\\n",
    "                    .sortByKey(ascending=False)\n",
    "adults_sorted = adults_rdd.map(lambda x: [int(x[11]), x[0]]) \\\n",
    "                    .sortByKey(ascending=False)\n",
    "oldies_sorted = oldies_rdd.map(lambda x: [int(x[11]), x[0]]) \\\n",
    "                    .sortByKey(ascending=False)\n",
    "\n",
    "# Count the number of rows in the filtered RDD\n",
    "print(f\"Number of rows in the children RDD: {children_sorted.count()}\")\n",
    "print(f\"Number of rows in the young_adults RDD: {young_adults_sorted.count()}\")\n",
    "print(f\"Number of rows in the adults RDD: {adults_sorted.count()}\")\n",
    "print(f\"Number of rows in the oldies RDD: {oldies_sorted.count()}\")\n",
    "\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ab073d-2f6e-4e05-b14c-53bfecf43173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Instances: 4\n",
      "Executor Memory: 2g\n",
      "Executor Cores: 4"
     ]
    }
   ],
   "source": [
    "# Access configuration\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print relevant executor settings\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
