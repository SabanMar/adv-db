{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a434b1-2d31-4d34-88e4-1c978c9e40e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2729</td><td>application_1732639283265_2688</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2688/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2688_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '1', 'spark.executor.memory': '1g', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2601</td><td>application_1732639283265_2560</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2560/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2560_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2704</td><td>application_1732639283265_2663</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2663/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2663_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2715</td><td>application_1732639283265_2674</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2674/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2674_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2719</td><td>application_1732639283265_2678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2678/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2678_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2726</td><td>application_1732639283265_2685</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2685/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2685_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2727</td><td>application_1732639283265_2686</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2686/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2686_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2728</td><td>application_1732639283265_2687</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2687/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2687_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>2729</td><td>application_1732639283265_2688</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2688/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2688_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"1\",\n",
    "        \"spark.executor.memory\": \"1g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7ed380e-2fea-45f0-99b1-38ca494181ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ZIP: string (nullable = true)\n",
      " |-- Comm: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|ZIP  |Comm                                                                                                   |income |\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|90001|Los Angeles (South Los Angeles), Florence-Graham                                                       |$33,887|\n",
      "|90002|Los Angeles (Southeast Los Angeles, Watts)                                                             |$30,413|\n",
      "|90003|Los Angeles (South Los Angeles, Southeast Los Angeles)                                                 |$30,805|\n",
      "|90004|Los Angeles (Hancock Park, Rampart Village, Virgil Village, Wilshire Center, Windsor Square)           |$40,612|\n",
      "|90005|Los Angeles (Hancock Park, Koreatown, Wilshire Center, Wilshire Park, Windsor Square)                  |$31,142|\n",
      "|90006|Los Angeles (Byzantine-Latino Quarter, Harvard Heights, Koreatown, Pico Heights)                       |$31,521|\n",
      "|90007|Los Angeles (Southeast Los Angeles, Univerity Park)                                                    |$22,304|\n",
      "|90008|Los Angeles (Baldwin Hills, Crenshaw, Leimert Park)                                                    |$36,564|\n",
      "|90010|Los Angeles (Hancock Park, Wilshire Center, Windsor Square)                                            |$45,786|\n",
      "|90011|Los Angeles (Southeast Los Angeles)                                                                    |$30,251|\n",
      "|90012|Los Angeles (Downtown Civic Center, Chinatown, Arts District, Bunker Hill, Historic Core, Little Tokyo)|$31,576|\n",
      "|90013|Los Angeles (Downtown Central, Downtown Fashion District)                                              |$19,887|\n",
      "|90014|Los Angeles (Downtown Historic Core, Arts District)                                                    |$23,642|\n",
      "|90015|Los Angeles (Dowtown Fashion District, South Park-South)                                               |$29,684|\n",
      "|90016|Los Angeles (West Adams)                                                                               |$38,330|\n",
      "|90017|Los Angeles (Downtown Bunker Hill, City West, South Park-North)                                        |$22,754|\n",
      "|90018|Los Angeles (Jefferson Park, Leimert Park)                                                             |$33,864|\n",
      "|90019|Los Angeles (Arlington Heights, Country Club Park, Mid-City)                                           |$46,571|\n",
      "|90020|Los Angeles (Hancock Park, Western Wilton, Wilshire Center, Windsor Square)                            |$38,849|\n",
      "|90021|Los Angeles (Downtown Fashion District, Downtown Southeast)                                            |$12,813|\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 7.35 seconds"
     ]
    }
   ],
   "source": [
    "# Spark Dataframe code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"query3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"ZIP\", StringType(), True),\n",
    "    StructField(\"Comm\", StringType(), True),\n",
    "    StructField(\"income\", StringType(), True),\n",
    "])\n",
    "\n",
    "income_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .schema(income_schema) \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "\n",
    "income_df.printSchema()\n",
    "income_df.show(20, truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a53339-e19d-4247-813b-0f10dd57f693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- BG10: string (nullable = true)\n",
      " |    |-- BG10FIP10: string (nullable = true)\n",
      " |    |-- BG12: string (nullable = true)\n",
      " |    |-- CB10: string (nullable = true)\n",
      " |    |-- CEN_FIP13: string (nullable = true)\n",
      " |    |-- CITY: string (nullable = true)\n",
      " |    |-- CITYCOM: string (nullable = true)\n",
      " |    |-- COMM: string (nullable = true)\n",
      " |    |-- CT10: string (nullable = true)\n",
      " |    |-- CT12: string (nullable = true)\n",
      " |    |-- CTCB10: string (nullable = true)\n",
      " |    |-- HD_2012: long (nullable = true)\n",
      " |    |-- HD_NAME: string (nullable = true)\n",
      " |    |-- HOUSING10: long (nullable = true)\n",
      " |    |-- LA_FIP10: string (nullable = true)\n",
      " |    |-- OBJECTID: long (nullable = true)\n",
      " |    |-- POP_2010: long (nullable = true)\n",
      " |    |-- PUMA10: string (nullable = true)\n",
      " |    |-- SPA_2012: long (nullable = true)\n",
      " |    |-- SPA_NAME: string (nullable = true)\n",
      " |    |-- SUP_DIST: string (nullable = true)\n",
      " |    |-- SUP_LABEL: string (nullable = true)\n",
      " |    |-- ShapeSTArea: double (nullable = true)\n",
      " |    |-- ShapeSTLength: double (nullable = true)\n",
      " |    |-- ZCTA10: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "|     _corrupt_record|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "|                   {|                NULL|                NULL|   NULL|\n",
      "|\"type\": \"FeatureC...|                NULL|                NULL|   NULL|\n",
      "|\"name\": \"2010_Cen...|                NULL|                NULL|   NULL|\n",
      "|\"crs\": { \"type\": ...|                NULL|                NULL|   NULL|\n",
      "|       \"features\": [|                NULL|                NULL|   NULL|\n",
      "|                NULL|{[[[-118.76689433...|{9012091, 9012091...|Feature|\n",
      "|                NULL|{[[[-118.88381235...|{9012091, 9012091...|Feature|\n",
      "|                NULL|{[[[-118.88937135...|{9012091, 9012091...|Feature|\n",
      "|                NULL|{[[[-117.77494517...|{9800031, 9800031...|Feature|\n",
      "|                NULL|{[[[-117.82891818...|{9800031, 9800031...|Feature|\n",
      "+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "census_schema = StructType([\n",
    "    StructField(\"CT10\", StringType(), True),\n",
    "    StructField(\"BG10\", StringType(), True),\n",
    "    StructField(\"CB10\", StringType(), True),\n",
    "    StructField(\"CTCB10\", StringType(), True),\n",
    "    StructField(\"BG10FIP10\", StringType(), True),\n",
    "    StructField(\"CEN_FIP13\", StringType(), True),\n",
    "    StructField(\"LA_FIP10\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"COMM\", StringType(), True),\n",
    "    StructField(\"CITYCOM\", StringType(), True),\n",
    "    StructField(\"ZCTA10\", StringType(), True),\n",
    "    StructField(\"PUMA10\", StringType(), True),\n",
    "    StructField(\"HD_2012\", StringType(), True),\n",
    "    StructField(\"HD_NAME\", StringType(), True),\n",
    "    StructField(\"SPA_2012\", StringType(), True),\n",
    "    StructField(\"SPA_NAME\", StringType(), True),\n",
    "    StructField(\"SUP_DIST\", StringType(), True),\n",
    "    StructField(\"SUP_LABEL\", StringType(), True),\n",
    "    StructField(\"HOUSING10\", IntegerType(), True),\n",
    "    StructField(\"POP_2010\", IntegerType(), True),\n",
    "    StructField(\"CT12\", StringType(), True),\n",
    "    StructField(\"BG12\", StringType(), True),\n",
    "    StructField(\"Shape.STArea()\", FloatType(), True),\n",
    "    StructField(\"Shape.STLength()\", FloatType(), True)\n",
    "])\n",
    "\n",
    "census_df = spark.read.format(\"json\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df.printSchema()\n",
    "census_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1d04ee2-096e-4bb4-96f3-456fbab5e05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+------------+---------------+\n",
      "|              COMM|            geometry|            ZIPCodes|TotalHousing|TotalPopulation|\n",
      "+------------------+--------------------+--------------------+------------+---------------+\n",
      "|    Toluca Terrace|POLYGON ((-118.35...|             [91601]|         541|           1301|\n",
      "|      Elysian Park|MULTIPOLYGON (((-...|      [90026, 90012]|        1992|           5261|\n",
      "|          Longwood|MULTIPOLYGON (((-...|             [90016]|        1474|           4210|\n",
      "|     Green Meadows|POLYGON ((-118.27...|[90001, 90003, 90...|        5204|          19814|\n",
      "|  Cadillac-Corning|POLYGON ((-118.37...|      [90034, 90035]|        2215|           6665|\n",
      "|          Mid-city|MULTIPOLYGON (((-...|             [90019]|        6692|          14339|\n",
      "|   Lincoln Heights|MULTIPOLYGON (((-...|[90065, 90032, 90...|        9197|          31101|\n",
      "|          Van Nuys|MULTIPOLYGON (((-...|[91402, 91406, 91...|       29170|          85959|\n",
      "|    Gramercy Place|POLYGON ((-118.31...|             [90047]|        3941|          10361|\n",
      "| Faircrest Heights|MULTIPOLYGON (((-...|[90019, 90016, 90...|        1356|           3436|\n",
      "|     Boyle Heights|MULTIPOLYGON (((-...|[90063, 90033, 90...|       21546|          81949|\n",
      "|  Lafayette Square|POLYGON ((-118.33...|      [90019, 90016]|        1563|           4358|\n",
      "|     Granada Hills|MULTIPOLYGON (((-...|[91342, 91344, 91...|       18855|          55172|\n",
      "|       North Hills|POLYGON ((-118.46...|[91343, 91344, 91...|       16203|          56344|\n",
      "|        Northridge|MULTIPOLYGON (((-...|[91325, 91324, 91...|       23329|          62225|\n",
      "|   Wilshire Center|MULTIPOLYGON (((-...|[90010, 90006, 90...|       19834|          47243|\n",
      "|    Jefferson Park|POLYGON ((-118.32...|             [90018]|        2306|           7612|\n",
      "|    Vermont Square|POLYGON ((-118.30...|      [90062, 90037]|        1953|           7045|\n",
      "|Cloverdale/Cochran|MULTIPOLYGON (((-...|      [90019, 90016]|        4984|          14030|\n",
      "|   Adams-Normandie|POLYGON ((-118.30...|      [90007, 90018]|        2424|           7842|\n",
      "+------------------+--------------------+--------------------+------------+---------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import collect_set,sum\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Read the file from s3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "\n",
    "la_df = flattened_df.filter(\n",
    "    (col(\"CITY\") == \"Los Angeles\") &\n",
    "    (col(\"HOUSING10\").isNotNull()) & (col(\"HOUSING10\") > 0) &\n",
    "    (col(\"POP_2010\").isNotNull()) & (col(\"POP_2010\") > 0)\n",
    ")\n",
    "\n",
    "\n",
    "LA_areas = la_df \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\"),   # Aggregate geometries\n",
    "        collect_set(\"ZCTA10\").alias(\"ZIPCodes\"),       # Collect unique ZIP codes as a list for each area\n",
    "        sum(\"HOUSING10\").alias(\"TotalHousing\"),        # Sum of HOUSING10 for each community\n",
    "        sum(\"POP_2010\").alias(\"TotalPopulation\")       # Sum of POP_2010 for each community\n",
    "    )\n",
    "\n",
    "LA_areas.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca8df04-56d3-4a81-8dfd-e14bd1f68708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------------------------------------------------+----------+\n",
      "|Community         |ZIPCodes                                                                |CrimeCount|\n",
      "+------------------+------------------------------------------------------------------------+----------+\n",
      "|North Hollywood   |[[91601, 91604, 91605, 91352, 91607, 91602, 91606]]                     |96565     |\n",
      "|Hollywood         |[[90046, 90027, 90038, 90028, 90069]]                                   |86140     |\n",
      "|Van Nuys          |[[91402, 91406, 91405, 91411, 91401]]                                   |63459     |\n",
      "|Melrose           |[[90048, 90046, 90036, 90038, 90004, 90069, 90029]]                     |59302     |\n",
      "|Sherman Oaks      |[[90210, 91604, 90077, 91607, 91403, 91411, 91401, 91423]]              |52555     |\n",
      "|West Vernon       |[[90062, 90037, 90047]]                                                 |52195     |\n",
      "|Boyle Heights     |[[90063, 90033, 90023]]                                                 |50578     |\n",
      "|San Pedro         |[[90732, 90744, 90731]]                                                 |50365     |\n",
      "|Vermont Vista     |[[90044, 90003]]                                                        |47099     |\n",
      "|Florence-Firestone|[[90001, 90011, 90003]]                                                 |46208     |\n",
      "|Wholesale District|[[90058, 90011, 90033, 90021, 90023, 90013, 90031, 90015, 90014, 90012]]|45021     |\n",
      "|Westlake          |[[90017, 90057, 90015]]                                                 |43672     |\n",
      "|Northridge        |[[91325, 91324, 91330, 91343, 91311]]                                   |43024     |\n",
      "|Downtown          |[[90017, 90007, 90013, 90015, 90014, 90012]]                            |42998     |\n",
      "|Reseda            |[[91325, 91316, 91335]]                                                 |39659     |\n",
      "|Wilshire Center   |[[90010, 90006, 90020, 90005, 90004]]                                   |37668     |\n",
      "|Harvard Park      |[[90044, 90003, 90047]]                                                 |36655     |\n",
      "|Watts             |[[90059, 90002]]                                                        |36450     |\n",
      "|Sylmar            |[[91340, 91342]]                                                        |35481     |\n",
      "|Panorama City     |[[91402, 91331]]                                                        |35338     |\n",
      "+------------------+------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from sedona.sql import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define crime data schema\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1-2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict_Age\", StringType(), True),\n",
    "    StructField(\"Vict_Sex\", StringType(), True),\n",
    "    StructField(\"Vict_Descent\", StringType(), True),\n",
    "    StructField(\"Premis_Cd\", StringType(), True),\n",
    "    StructField(\"Premis_Desc\", StringType(), True),\n",
    "    StructField(\"Weapon_Used_Cd\", StringType(), True),\n",
    "    StructField(\"Weapon_Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status_Desc\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_1\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_3\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_4\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross_Street\", StringType(), True),\n",
    "    StructField(\"LAT\", StringType(), True),\n",
    "    StructField(\"LON\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read crime data\n",
    "crimes_df1 = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crimes_df2 = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "# Combine datasets\n",
    "crimes_df = crimes_df1.union(crimes_df2)\n",
    "\n",
    "# Filter valid coordinates and create geometry column for crimes\n",
    "crimes_df = crimes_df.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "crimes_df = crimes_df.withColumn(\"geometry\", ST_Point(col(\"LON\").cast(\"double\"), col(\"LAT\").cast(\"double\")))\n",
    "\n",
    "\n",
    "# Perform spatial join: match crime locations with LA areas\n",
    "crimes_within_areas = crimes_df.alias(\"crimes\").join(\n",
    "    LA_areas.alias(\"LA_areas\"),\n",
    "    ST_Contains(col(\"LA_areas.geometry\"), col(\"crimes.geometry\")),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Count the number of crimes in each area and collect ZIP codes\n",
    "crime_counts_with_zip = crimes_within_areas.groupBy(\n",
    "    col(\"LA_areas.COMM\").alias(\"Community\")\n",
    ").agg(\n",
    "    collect_set(\"LA_areas.ZIPCodes\").alias(\"ZIPCodes\"),  # Collect ZIP codes\n",
    "    count(\"*\").alias(\"CrimeCount\")                    # Count crimes\n",
    ").orderBy(col(\"CrimeCount\").desc())\n",
    "\n",
    "# Flatten the collected ZIPCodes list for better readability\n",
    "crime_counts_with_zip = crime_counts_with_zip.withColumn(\n",
    "    \"ZIPCodes\", col(\"ZIPCodes\").cast(\"string\")  # Convert list of ZIPs to string format\n",
    ")\n",
    "\n",
    "# Show the crime counts with ZIP codes\n",
    "crime_counts_with_zip.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58c2e0e1-e4d7-43cc-a573-b89ae0c12648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+---------------+---------------+\n",
      "|Community         |CrimeCount|TotalPopulation|CrimesPerCapita|\n",
      "+------------------+----------+---------------+---------------+\n",
      "|North Hollywood   |96565     |142347         |0.678377       |\n",
      "|Hollywood         |86140     |62213          |1.384598       |\n",
      "|Van Nuys          |63459     |85959          |0.738247       |\n",
      "|Melrose           |59302     |73442          |0.807467       |\n",
      "|Sherman Oaks      |52555     |81334          |0.646163       |\n",
      "|West Vernon       |52195     |50300          |1.037674       |\n",
      "|Boyle Heights     |50578     |81949          |0.617189       |\n",
      "|San Pedro         |50365     |73394          |0.686228       |\n",
      "|Vermont Vista     |47099     |37525          |1.255137       |\n",
      "|Florence-Firestone|46208     |43638          |1.058894       |\n",
      "|Wholesale District|45021     |29156          |1.544142       |\n",
      "|Westlake          |43672     |54626          |0.799473       |\n",
      "|Northridge        |43024     |62225          |0.691426       |\n",
      "|Downtown          |42998     |21924          |1.96123        |\n",
      "|Reseda            |39659     |71818          |0.552215       |\n",
      "|Wilshire Center   |37668     |47243          |0.797324       |\n",
      "|Harvard Park      |36655     |36442          |1.005845       |\n",
      "|Watts             |36450     |39652          |0.919247       |\n",
      "|Sylmar            |35481     |77360          |0.458648       |\n",
      "|Panorama City     |35338     |69500          |0.50846        |\n",
      "+------------------+----------+---------------+---------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Join crime counts with population data\n",
    "crime_with_population = crime_counts_with_zip.join(\n",
    "    LA_areas,\n",
    "    crime_counts_with_zip[\"Community\"] == LA_areas[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimeCount\"),\n",
    "    col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "crime_with_population = crime_with_population.withColumn(\n",
    "    \"CrimesPerCapita\",\n",
    "    round(col(\"CrimeCount\") / col(\"TotalPopulation\"), 6)  # Round to 6 decimal places for clarity\n",
    ")\n",
    "\n",
    "# Show all results sorted by CrimeCount in descending order\n",
    "crime_with_population.select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimeCount\"),\n",
    "    col(\"TotalPopulation\"),\n",
    "    col(\"CrimesPerCapita\")\n",
    ").orderBy(col(\"CrimeCount\").desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a462fd4b-082e-45c7-a3eb-90368522f802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+--------------+\n",
      "|ZIP_Income|           Community| income|income_cleaned|\n",
      "+----------+--------------------+-------+--------------+\n",
      "|     90001|Los Angeles (Sout...|$33,887|       33887.0|\n",
      "|     90002|Los Angeles (Sout...|$30,413|       30413.0|\n",
      "|     90003|Los Angeles (Sout...|$30,805|       30805.0|\n",
      "|     90004|Los Angeles (Hanc...|$40,612|       40612.0|\n",
      "|     90005|Los Angeles (Hanc...|$31,142|       31142.0|\n",
      "|     90006|Los Angeles (Byza...|$31,521|       31521.0|\n",
      "|     90007|Los Angeles (Sout...|$22,304|       22304.0|\n",
      "|     90008|Los Angeles (Bald...|$36,564|       36564.0|\n",
      "|     90010|Los Angeles (Hanc...|$45,786|       45786.0|\n",
      "|     90011|Los Angeles (Sout...|$30,251|       30251.0|\n",
      "|     90012|Los Angeles (Down...|$31,576|       31576.0|\n",
      "|     90013|Los Angeles (Down...|$19,887|       19887.0|\n",
      "|     90014|Los Angeles (Down...|$23,642|       23642.0|\n",
      "|     90015|Los Angeles (Dowt...|$29,684|       29684.0|\n",
      "|     90016|Los Angeles (West...|$38,330|       38330.0|\n",
      "|     90017|Los Angeles (Down...|$22,754|       22754.0|\n",
      "|     90018|Los Angeles (Jeff...|$33,864|       33864.0|\n",
      "|     90019|Los Angeles (Arli...|$46,571|       46571.0|\n",
      "|     90020|Los Angeles (Hanc...|$38,849|       38849.0|\n",
      "|     90021|Los Angeles (Down...|$12,813|       12813.0|\n",
      "+----------+--------------------+-------+--------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the income column in income_df\n",
    "income_df_cleaned = income_df.withColumn(\n",
    "    \"income_cleaned\",\n",
    "    regexp_replace(col(\"income\"), \"[$,]\", \"\").cast(\"double\")  # Remove dollar signs and commas, cast to double\n",
    ")\n",
    "income_df_LA = income_df_cleaned.filter(col(\"Comm\").contains(\"Los Angeles\"))\n",
    "# Rename the columns to avoid ambiguity\n",
    "income_df_LA = income_df_LA.withColumnRenamed(\"Comm\", \"Community\")\n",
    "income_df_LA = income_df_LA.withColumnRenamed(\"ZIP\", \"ZIP_Income\")\n",
    "income_df_LA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8501ed-b5c3-48fc-a486-581d9e8c4f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+---------------+\n",
      "|Community              |CrimesPerCapita|IncomePerCapita|\n",
      "+-----------------------+---------------+---------------+\n",
      "|Beverly Crest          |0.368961       |80637.577      |\n",
      "|Pacific Palisades      |0.379676       |71315.178      |\n",
      "|Bel Air                |0.399225       |70879.469      |\n",
      "|Venice                 |1.040428       |67888.88       |\n",
      "|Palisades Highlands    |0.187842       |67392.081      |\n",
      "|Marina Peninsula       |0.599954       |63010.679      |\n",
      "|Mandeville Canyon      |0.261058       |62110.61       |\n",
      "|Brentwood              |0.405864       |56551.715      |\n",
      "|Playa Vista            |0.500448       |55354.295      |\n",
      "|Carthay                |0.762896       |47221.492      |\n",
      "|Playa Del Rey          |0.742559       |45486.718      |\n",
      "|Hollywood Hills        |0.747616       |41672.817      |\n",
      "|Studio City            |0.783407       |41151.145      |\n",
      "|Century City           |0.632969       |40671.613      |\n",
      "|South Carthay          |0.723174       |39566.826      |\n",
      "|West Los Angeles       |0.619004       |38641.406      |\n",
      "|Rancho Park            |1.008644       |37349.171      |\n",
      "|Angeles National Forest|0.411765       |36448.933      |\n",
      "|Encino                 |0.623781       |36441.693      |\n",
      "|Woodland Hills         |0.545094       |36285.191      |\n",
      "+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken (default join): 33.56 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, sum\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# avg_income for each community (each community has multiple zip codes)\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "result_df = aggregated_df.join(\n",
    "    crime_with_population,\n",
    "    crime_with_population[\"Community\"] == aggregated_df[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimesPerCapita\"),\n",
    "    col(\"IncomePerCapita\"),\n",
    ")\n",
    "\n",
    "# Sort the results in descending order by IncomePerCapita\n",
    "result_df = result_df.orderBy(col(\"IncomePerCapita\").desc())\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken (default join): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84ad2632-1b91-4027-a490-285c8a64cc12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (55)\n",
      "+- Sort (54)\n",
      "   +- Exchange (53)\n",
      "      +- Project (52)\n",
      "         +- SortMergeJoin Inner (51)\n",
      "            :- Sort (15)\n",
      "            :  +- HashAggregate (14)\n",
      "            :     +- Exchange (13)\n",
      "            :        +- HashAggregate (12)\n",
      "            :           +- Project (11)\n",
      "            :              +- BroadcastHashJoin Inner BuildRight (10)\n",
      "            :                 :- Project (5)\n",
      "            :                 :  +- Filter (4)\n",
      "            :                 :     +- Generate (3)\n",
      "            :                 :        +- Filter (2)\n",
      "            :                 :           +- Scan geojson  (1)\n",
      "            :                 +- BroadcastExchange (9)\n",
      "            :                    +- Project (8)\n",
      "            :                       +- Filter (7)\n",
      "            :                          +- Scan csv  (6)\n",
      "            +- Filter (50)\n",
      "               +- Project (49)\n",
      "                  +- SortMergeJoin Inner (48)\n",
      "                     :- Sort (38)\n",
      "                     :  +- HashAggregate (37)\n",
      "                     :     +- Exchange (36)\n",
      "                     :        +- HashAggregate (35)\n",
      "                     :           +- Filter (34)\n",
      "                     :              +- Project (33)\n",
      "                     :                 +- RangeJoin (32)\n",
      "                     :                    :- Union (22)\n",
      "                     :                    :  :- Project (18)\n",
      "                     :                    :  :  +- Filter (17)\n",
      "                     :                    :  :     +- Scan csv  (16)\n",
      "                     :                    :  +- Project (21)\n",
      "                     :                    :     +- Filter (20)\n",
      "                     :                    :        +- Scan csv  (19)\n",
      "                     :                    +- Filter (31)\n",
      "                     :                       +- ObjectHashAggregate (30)\n",
      "                     :                          +- Exchange (29)\n",
      "                     :                             +- ObjectHashAggregate (28)\n",
      "                     :                                +- Project (27)\n",
      "                     :                                   +- Filter (26)\n",
      "                     :                                      +- Generate (25)\n",
      "                     :                                         +- Filter (24)\n",
      "                     :                                            +- Scan geojson  (23)\n",
      "                     +- Sort (47)\n",
      "                        +- HashAggregate (46)\n",
      "                           +- Exchange (45)\n",
      "                              +- HashAggregate (44)\n",
      "                                 +- Project (43)\n",
      "                                    +- Filter (42)\n",
      "                                       +- Generate (41)\n",
      "                                          +- Filter (40)\n",
      "                                             +- Scan geojson  (39)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [3]: [ZIP#0, Comm#1, income#2]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Comm), StringContains(Comm,Los Angeles), IsNotNull(ZIP)]\n",
      "ReadSchema: struct<ZIP:string,Comm:string,income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "Condition : ((isnotnull(Comm#1) AND Contains(Comm#1, Los Angeles)) AND isnotnull(ZIP#0))\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [ZIP#0 AS ZIP_Income#760, cast(regexp_replace(income#2, [$,], , 1) as double) AS income_cleaned#750]\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [2]: [ZIP_Income#760, income_cleaned#750]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2798]\n",
      "\n",
      "(10) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#127]\n",
      "Right keys [1]: [ZIP_Income#760]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [COMM#110, ((income_cleaned#750 * cast(HOUSING10#116L as double)) / cast(POP_2010#119L as double)) AS IncomePerCapita_zip#848]\n",
      "Input [6]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127, ZIP_Income#760, income_cleaned#750]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [2]: [COMM#110, IncomePerCapita_zip#848]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_avg(IncomePerCapita_zip#848)]\n",
      "Aggregate Attributes [2]: [sum#959, count#960L]\n",
      "Results [3]: [COMM#110, sum#961, count#962L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [3]: [COMM#110, sum#961, count#962L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=2803]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [3]: [COMM#110, sum#961, count#962L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [avg(IncomePerCapita_zip#848)]\n",
      "Aggregate Attributes [1]: [avg(IncomePerCapita_zip#848)#911]\n",
      "Results [2]: [COMM#110, round(avg(IncomePerCapita_zip#848)#911, 3) AS IncomePerCapita#912]\n",
      "\n",
      "(15) Sort\n",
      "Input [2]: [COMM#110, IncomePerCapita#912]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#361, LON#362]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "Condition : ((isnotnull(LAT#361) AND isnotnull(LON#362)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#475]\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#417, LON#418]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "Condition : ((isnotnull(LAT#417) AND isnotnull(LON#418)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1015]\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan geojson \n",
      "Output [1]: [features#916]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#916]\n",
      "Condition : ((size(features#916, true) > 0) AND isnotnull(features#916))\n",
      "\n",
      "(25) Generate\n",
      "Input [1]: [features#916]\n",
      "Arguments: explode(features#916), false, [features#94]\n",
      "\n",
      "(26) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.geometry AS geometry#97]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(28) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, geometry#97]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#300]\n",
      "Results [2]: [COMM#110, buf#301]\n",
      "\n",
      "(29) Exchange\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=2805]\n",
      "\n",
      "(30) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#97)#260]\n",
      "Results [2]: [COMM#110, ST_Union_Aggr(geometry#97)#260 AS geometry#261]\n",
      "\n",
      "(31) Filter\n",
      "Input [2]: [COMM#110, geometry#261]\n",
      "Condition : isnotnull(geometry#261)\n",
      "\n",
      "(32) RangeJoin\n",
      "Arguments: geometry#475: geometry, geometry#261: geometry, WITHIN\n",
      "\n",
      "(33) Project\n",
      "Output [1]: [COMM#110]\n",
      "Input [3]: [geometry#475, COMM#110, geometry#261]\n",
      "\n",
      "(34) Filter\n",
      "Input [1]: [COMM#110]\n",
      "Condition : bloomfilter#1017 of [bf1017 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#110]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#631L]\n",
      "Results [2]: [COMM#110, count#632L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=2966]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#610L]\n",
      "Results [2]: [COMM#110 AS Community#573, count(1)#610L AS CrimeCount#611L]\n",
      "\n",
      "(38) Sort\n",
      "Input [2]: [Community#573, CrimeCount#611L]\n",
      "Arguments: [Community#573 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(39) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(40) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(41) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(42) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(43) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(44) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(45) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=2815]\n",
      "\n",
      "(46) HashAggregate\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum(POP_2010#119L)#266L]\n",
      "Results [2]: [COMM#110, sum(POP_2010#119L)#266L AS TotalPopulation#267L]\n",
      "\n",
      "(47) Sort\n",
      "Input [2]: [COMM#110, TotalPopulation#267L]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(48) SortMergeJoin\n",
      "Left keys [1]: [Community#573]\n",
      "Right keys [1]: [COMM#110]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(49) Project\n",
      "Output [2]: [Community#573, round((cast(CrimeCount#611L as double) / cast(TotalPopulation#267L as double)), 6) AS CrimesPerCapita#695]\n",
      "Input [4]: [Community#573, CrimeCount#611L, COMM#110, TotalPopulation#267L]\n",
      "\n",
      "(50) Filter\n",
      "Input [2]: [Community#573, CrimesPerCapita#695]\n",
      "Condition : true\n",
      "\n",
      "(51) SortMergeJoin\n",
      "Left keys [1]: [COMM#110]\n",
      "Right keys [1]: [Community#573]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(52) Project\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#912]\n",
      "Input [4]: [COMM#110, IncomePerCapita#912, Community#573, CrimesPerCapita#695]\n",
      "\n",
      "(53) Exchange\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#912]\n",
      "Arguments: rangepartitioning(IncomePerCapita#912 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=2975]\n",
      "\n",
      "(54) Sort\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#912]\n",
      "Arguments: [IncomePerCapita#912 DESC NULLS LAST], true, 0\n",
      "\n",
      "(55) AdaptiveSparkPlan\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#912]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 34 Hosting Expression = bloomfilter#1017 of [bf1017 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "OutputAdapter (64)\n",
      "+- AdaptiveSparkPlan (63)\n",
      "   +- Exchange (62)\n",
      "      +- HashAggregate (61)\n",
      "         +- Project (60)\n",
      "            +- Filter (59)\n",
      "               +- Generate (58)\n",
      "                  +- Filter (57)\n",
      "                     +- Scan geojson  (56)\n",
      "\n",
      "\n",
      "(56) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(57) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(58) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(59) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(60) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(61) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(62) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=2960]\n",
      "\n",
      "(63) AdaptiveSparkPlan\n",
      "Output [2]: [COMM#110, sum#307L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(64) OutputAdapter\n",
      "Output [2]: [COMM#110, sum#307L]"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b760b8f-f76d-4129-8bb5-2ff3b0dca00f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+---------------+\n",
      "|Community              |CrimesPerCapita|IncomePerCapita|\n",
      "+-----------------------+---------------+---------------+\n",
      "|Beverly Crest          |0.368961       |80637.577      |\n",
      "|Pacific Palisades      |0.379676       |71315.178      |\n",
      "|Bel Air                |0.399225       |70879.469      |\n",
      "|Venice                 |1.040428       |67888.88       |\n",
      "|Palisades Highlands    |0.187842       |67392.081      |\n",
      "|Marina Peninsula       |0.599954       |63010.679      |\n",
      "|Mandeville Canyon      |0.261058       |62110.61       |\n",
      "|Brentwood              |0.405864       |56551.715      |\n",
      "|Playa Vista            |0.500448       |55354.295      |\n",
      "|Carthay                |0.762896       |47221.492      |\n",
      "|Playa Del Rey          |0.742559       |45486.718      |\n",
      "|Hollywood Hills        |0.747616       |41672.817      |\n",
      "|Studio City            |0.783407       |41151.145      |\n",
      "|Century City           |0.632969       |40671.613      |\n",
      "|South Carthay          |0.723174       |39566.826      |\n",
      "|West Los Angeles       |0.619004       |38641.406      |\n",
      "|Rancho Park            |1.008644       |37349.171      |\n",
      "|Angeles National Forest|0.411765       |36448.933      |\n",
      "|Encino                 |0.623781       |36441.693      |\n",
      "|Woodland Hills         |0.545094       |36285.191      |\n",
      "+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken (broadcast join): 54.29 seconds"
     ]
    }
   ],
   "source": [
    "# BROADCAST JOIN\n",
    "from pyspark.sql.functions import explode, col, sum\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.hint(\"broadcast\").join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip. AvgPpeoplePerHouse = POP_2010 / HOUSING10 \n",
    "# IncomePerCapita_zip = IncomePerHouse / AvgPpeoplePerHouse = IncomePerHouse * HOUSING10 / POP_2010\n",
    "\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# IncomePerCapita = avg(IncomePerCapita_zip) for each community (each community has multiple zip codes)\n",
    "\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "result_df = aggregated_df.hint(\"broadcast\").join(\n",
    "    crime_with_population,\n",
    "    crime_with_population[\"Community\"] == aggregated_df[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimesPerCapita\"),\n",
    "    col(\"IncomePerCapita\"),\n",
    ")\n",
    "\n",
    "# Sort the results in descending order by IncomePerCapita\n",
    "result_df = result_df.orderBy(col(\"IncomePerCapita\").desc())\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken (broadcast join): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23df4fa5-be0f-46ac-b013-108ba40e00a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (54)\n",
      "+- Sort (53)\n",
      "   +- Exchange (52)\n",
      "      +- Project (51)\n",
      "         +- BroadcastHashJoin Inner BuildLeft (50)\n",
      "            :- BroadcastExchange (15)\n",
      "            :  +- HashAggregate (14)\n",
      "            :     +- Exchange (13)\n",
      "            :        +- HashAggregate (12)\n",
      "            :           +- Project (11)\n",
      "            :              +- BroadcastHashJoin Inner BuildLeft (10)\n",
      "            :                 :- BroadcastExchange (6)\n",
      "            :                 :  +- Project (5)\n",
      "            :                 :     +- Filter (4)\n",
      "            :                 :        +- Generate (3)\n",
      "            :                 :           +- Filter (2)\n",
      "            :                 :              +- Scan geojson  (1)\n",
      "            :                 +- Project (9)\n",
      "            :                    +- Filter (8)\n",
      "            :                       +- Scan csv  (7)\n",
      "            +- Project (49)\n",
      "               +- SortMergeJoin Inner (48)\n",
      "                  :- Sort (38)\n",
      "                  :  +- HashAggregate (37)\n",
      "                  :     +- Exchange (36)\n",
      "                  :        +- HashAggregate (35)\n",
      "                  :           +- Filter (34)\n",
      "                  :              +- Project (33)\n",
      "                  :                 +- RangeJoin (32)\n",
      "                  :                    :- Union (22)\n",
      "                  :                    :  :- Project (18)\n",
      "                  :                    :  :  +- Filter (17)\n",
      "                  :                    :  :     +- Scan csv  (16)\n",
      "                  :                    :  +- Project (21)\n",
      "                  :                    :     +- Filter (20)\n",
      "                  :                    :        +- Scan csv  (19)\n",
      "                  :                    +- Filter (31)\n",
      "                  :                       +- ObjectHashAggregate (30)\n",
      "                  :                          +- Exchange (29)\n",
      "                  :                             +- ObjectHashAggregate (28)\n",
      "                  :                                +- Project (27)\n",
      "                  :                                   +- Filter (26)\n",
      "                  :                                      +- Generate (25)\n",
      "                  :                                         +- Filter (24)\n",
      "                  :                                            +- Scan geojson  (23)\n",
      "                  +- Sort (47)\n",
      "                     +- HashAggregate (46)\n",
      "                        +- Exchange (45)\n",
      "                           +- HashAggregate (44)\n",
      "                              +- Project (43)\n",
      "                                 +- Filter (42)\n",
      "                                    +- Generate (41)\n",
      "                                       +- Filter (40)\n",
      "                                          +- Scan geojson  (39)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[3, string, true]),false), [plan_id=4337]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [3]: [ZIP#0, Comm#1, income#2]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Comm), StringContains(Comm,Los Angeles), IsNotNull(ZIP)]\n",
      "ReadSchema: struct<ZIP:string,Comm:string,income:string>\n",
      "\n",
      "(8) Filter\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "Condition : ((isnotnull(Comm#1) AND Contains(Comm#1, Los Angeles)) AND isnotnull(ZIP#0))\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [ZIP#0 AS ZIP_Income#760, cast(regexp_replace(income#2, [$,], , 1) as double) AS income_cleaned#750]\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "\n",
      "(10) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#127]\n",
      "Right keys [1]: [ZIP_Income#760]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [COMM#110, ((income_cleaned#750 * cast(HOUSING10#116L as double)) / cast(POP_2010#119L as double)) AS IncomePerCapita_zip#1094]\n",
      "Input [6]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127, ZIP_Income#760, income_cleaned#750]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [2]: [COMM#110, IncomePerCapita_zip#1094]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_avg(IncomePerCapita_zip#1094)]\n",
      "Aggregate Attributes [2]: [sum#1205, count#1206L]\n",
      "Results [3]: [COMM#110, sum#1207, count#1208L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [3]: [COMM#110, sum#1207, count#1208L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=4342]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [3]: [COMM#110, sum#1207, count#1208L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [avg(IncomePerCapita_zip#1094)]\n",
      "Aggregate Attributes [1]: [avg(IncomePerCapita_zip#1094)#1157]\n",
      "Results [2]: [COMM#110, round(avg(IncomePerCapita_zip#1094)#1157, 3) AS IncomePerCapita#1158]\n",
      "\n",
      "(15) BroadcastExchange\n",
      "Input [2]: [COMM#110, IncomePerCapita#1158]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=4363]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#361, LON#362]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "Condition : ((isnotnull(LAT#361) AND isnotnull(LON#362)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#475]\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#417, LON#418]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "Condition : ((isnotnull(LAT#417) AND isnotnull(LON#418)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1265]\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan geojson \n",
      "Output [1]: [features#1162]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#1162]\n",
      "Condition : ((size(features#1162, true) > 0) AND isnotnull(features#1162))\n",
      "\n",
      "(25) Generate\n",
      "Input [1]: [features#1162]\n",
      "Arguments: explode(features#1162), false, [features#94]\n",
      "\n",
      "(26) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.geometry AS geometry#97]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(28) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, geometry#97]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#300]\n",
      "Results [2]: [COMM#110, buf#301]\n",
      "\n",
      "(29) Exchange\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=4344]\n",
      "\n",
      "(30) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#97)#260]\n",
      "Results [2]: [COMM#110, ST_Union_Aggr(geometry#97)#260 AS geometry#261]\n",
      "\n",
      "(31) Filter\n",
      "Input [2]: [COMM#110, geometry#261]\n",
      "Condition : isnotnull(geometry#261)\n",
      "\n",
      "(32) RangeJoin\n",
      "Arguments: geometry#475: geometry, geometry#261: geometry, WITHIN\n",
      "\n",
      "(33) Project\n",
      "Output [1]: [COMM#110]\n",
      "Input [3]: [geometry#475, COMM#110, geometry#261]\n",
      "\n",
      "(34) Filter\n",
      "Input [1]: [COMM#110]\n",
      "Condition : bloomfilter#1266 of [bf1266 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#110]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#631L]\n",
      "Results [2]: [COMM#110, count#632L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=4490]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#610L]\n",
      "Results [2]: [COMM#110 AS Community#573, count(1)#610L AS CrimeCount#611L]\n",
      "\n",
      "(38) Sort\n",
      "Input [2]: [Community#573, CrimeCount#611L]\n",
      "Arguments: [Community#573 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(39) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(40) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(41) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(42) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(43) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(44) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(45) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=4354]\n",
      "\n",
      "(46) HashAggregate\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum(POP_2010#119L)#266L]\n",
      "Results [2]: [COMM#110, sum(POP_2010#119L)#266L AS TotalPopulation#267L]\n",
      "\n",
      "(47) Sort\n",
      "Input [2]: [COMM#110, TotalPopulation#267L]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(48) SortMergeJoin\n",
      "Left keys [1]: [Community#573]\n",
      "Right keys [1]: [COMM#110]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(49) Project\n",
      "Output [2]: [Community#573, round((cast(CrimeCount#611L as double) / cast(TotalPopulation#267L as double)), 6) AS CrimesPerCapita#695]\n",
      "Input [4]: [Community#573, CrimeCount#611L, COMM#110, TotalPopulation#267L]\n",
      "\n",
      "(50) BroadcastHashJoin\n",
      "Left keys [1]: [COMM#110]\n",
      "Right keys [1]: [Community#573]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(51) Project\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1158]\n",
      "Input [4]: [COMM#110, IncomePerCapita#1158, Community#573, CrimesPerCapita#695]\n",
      "\n",
      "(52) Exchange\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1158]\n",
      "Arguments: rangepartitioning(IncomePerCapita#1158 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=4497]\n",
      "\n",
      "(53) Sort\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1158]\n",
      "Arguments: [IncomePerCapita#1158 DESC NULLS LAST], true, 0\n",
      "\n",
      "(54) AdaptiveSparkPlan\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1158]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 34 Hosting Expression = bloomfilter#1266 of [bf1266 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "OutputAdapter (63)\n",
      "+- AdaptiveSparkPlan (62)\n",
      "   +- Exchange (61)\n",
      "      +- HashAggregate (60)\n",
      "         +- Project (59)\n",
      "            +- Filter (58)\n",
      "               +- Generate (57)\n",
      "                  +- Filter (56)\n",
      "                     +- Scan geojson  (55)\n",
      "\n",
      "\n",
      "(55) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(56) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(57) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(58) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(59) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(60) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(61) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=4484]\n",
      "\n",
      "(62) AdaptiveSparkPlan\n",
      "Output [2]: [COMM#110, sum#307L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(63) OutputAdapter\n",
      "Output [2]: [COMM#110, sum#307L]"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27008ed9-421c-466d-815f-721c2e91aae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+---------------+\n",
      "|Community              |CrimesPerCapita|IncomePerCapita|\n",
      "+-----------------------+---------------+---------------+\n",
      "|Beverly Crest          |0.368961       |80637.577      |\n",
      "|Pacific Palisades      |0.379676       |71315.178      |\n",
      "|Bel Air                |0.399225       |70879.469      |\n",
      "|Venice                 |1.040428       |67888.88       |\n",
      "|Palisades Highlands    |0.187842       |67392.081      |\n",
      "|Marina Peninsula       |0.599954       |63010.679      |\n",
      "|Mandeville Canyon      |0.261058       |62110.61       |\n",
      "|Brentwood              |0.405864       |56551.715      |\n",
      "|Playa Vista            |0.500448       |55354.295      |\n",
      "|Carthay                |0.762896       |47221.492      |\n",
      "|Playa Del Rey          |0.742559       |45486.718      |\n",
      "|Hollywood Hills        |0.747616       |41672.817      |\n",
      "|Studio City            |0.783407       |41151.145      |\n",
      "|Century City           |0.632969       |40671.613      |\n",
      "|South Carthay          |0.723174       |39566.826      |\n",
      "|West Los Angeles       |0.619004       |38641.406      |\n",
      "|Rancho Park            |1.008644       |37349.171      |\n",
      "|Angeles National Forest|0.411765       |36448.933      |\n",
      "|Encino                 |0.623781       |36441.693      |\n",
      "|Woodland Hills         |0.545094       |36285.191      |\n",
      "+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken (merge join): 33.54 seconds"
     ]
    }
   ],
   "source": [
    "# MERGE JOIN\n",
    "from pyspark.sql.functions import explode, col, sum\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.hint(\"merge\").join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# avg_income for each community (each community has multiple zip codes)\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "result_df = aggregated_df.hint(\"merge\").join(\n",
    "    crime_with_population,\n",
    "    crime_with_population[\"Community\"] == aggregated_df[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimesPerCapita\"),\n",
    "    col(\"IncomePerCapita\"),\n",
    ")\n",
    "\n",
    "# Sort the results in descending order by IncomePerCapita\n",
    "result_df = result_df.orderBy(col(\"IncomePerCapita\").desc())\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken (merge join): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d92127b5-ea21-4595-9e74-60542cceffa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (58)\n",
      "+- Sort (57)\n",
      "   +- Exchange (56)\n",
      "      +- Project (55)\n",
      "         +- SortMergeJoin Inner (54)\n",
      "            :- Sort (18)\n",
      "            :  +- HashAggregate (17)\n",
      "            :     +- Exchange (16)\n",
      "            :        +- HashAggregate (15)\n",
      "            :           +- Project (14)\n",
      "            :              +- SortMergeJoin Inner (13)\n",
      "            :                 :- Sort (7)\n",
      "            :                 :  +- Exchange (6)\n",
      "            :                 :     +- Project (5)\n",
      "            :                 :        +- Filter (4)\n",
      "            :                 :           +- Generate (3)\n",
      "            :                 :              +- Filter (2)\n",
      "            :                 :                 +- Scan geojson  (1)\n",
      "            :                 +- Sort (12)\n",
      "            :                    +- Exchange (11)\n",
      "            :                       +- Project (10)\n",
      "            :                          +- Filter (9)\n",
      "            :                             +- Scan csv  (8)\n",
      "            +- Filter (53)\n",
      "               +- Project (52)\n",
      "                  +- SortMergeJoin Inner (51)\n",
      "                     :- Sort (41)\n",
      "                     :  +- HashAggregate (40)\n",
      "                     :     +- Exchange (39)\n",
      "                     :        +- HashAggregate (38)\n",
      "                     :           +- Filter (37)\n",
      "                     :              +- Project (36)\n",
      "                     :                 +- RangeJoin (35)\n",
      "                     :                    :- Union (25)\n",
      "                     :                    :  :- Project (21)\n",
      "                     :                    :  :  +- Filter (20)\n",
      "                     :                    :  :     +- Scan csv  (19)\n",
      "                     :                    :  +- Project (24)\n",
      "                     :                    :     +- Filter (23)\n",
      "                     :                    :        +- Scan csv  (22)\n",
      "                     :                    +- Filter (34)\n",
      "                     :                       +- ObjectHashAggregate (33)\n",
      "                     :                          +- Exchange (32)\n",
      "                     :                             +- ObjectHashAggregate (31)\n",
      "                     :                                +- Project (30)\n",
      "                     :                                   +- Filter (29)\n",
      "                     :                                      +- Generate (28)\n",
      "                     :                                         +- Filter (27)\n",
      "                     :                                            +- Scan geojson  (26)\n",
      "                     +- Sort (50)\n",
      "                        +- HashAggregate (49)\n",
      "                           +- Exchange (48)\n",
      "                              +- HashAggregate (47)\n",
      "                                 +- Project (46)\n",
      "                                    +- Filter (45)\n",
      "                                       +- Generate (44)\n",
      "                                          +- Filter (43)\n",
      "                                             +- Scan geojson  (42)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(6) Exchange\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: hashpartitioning(ZCTA10#127, 1000), ENSURE_REQUIREMENTS, [plan_id=6288]\n",
      "\n",
      "(7) Sort\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: [ZCTA10#127 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [ZIP#0, Comm#1, income#2]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Comm), StringContains(Comm,Los Angeles), IsNotNull(ZIP)]\n",
      "ReadSchema: struct<ZIP:string,Comm:string,income:string>\n",
      "\n",
      "(9) Filter\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "Condition : ((isnotnull(Comm#1) AND Contains(Comm#1, Los Angeles)) AND isnotnull(ZIP#0))\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [ZIP#0 AS ZIP_Income#760, cast(regexp_replace(income#2, [$,], , 1) as double) AS income_cleaned#750]\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "\n",
      "(11) Exchange\n",
      "Input [2]: [ZIP_Income#760, income_cleaned#750]\n",
      "Arguments: hashpartitioning(ZIP_Income#760, 1000), ENSURE_REQUIREMENTS, [plan_id=6289]\n",
      "\n",
      "(12) Sort\n",
      "Input [2]: [ZIP_Income#760, income_cleaned#750]\n",
      "Arguments: [ZIP_Income#760 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(13) SortMergeJoin\n",
      "Left keys [1]: [ZCTA10#127]\n",
      "Right keys [1]: [ZIP_Income#760]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [2]: [COMM#110, ((income_cleaned#750 * cast(HOUSING10#116L as double)) / cast(POP_2010#119L as double)) AS IncomePerCapita_zip#1344]\n",
      "Input [6]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127, ZIP_Income#760, income_cleaned#750]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [2]: [COMM#110, IncomePerCapita_zip#1344]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_avg(IncomePerCapita_zip#1344)]\n",
      "Aggregate Attributes [2]: [sum#1455, count#1456L]\n",
      "Results [3]: [COMM#110, sum#1457, count#1458L]\n",
      "\n",
      "(16) Exchange\n",
      "Input [3]: [COMM#110, sum#1457, count#1458L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=6296]\n",
      "\n",
      "(17) HashAggregate\n",
      "Input [3]: [COMM#110, sum#1457, count#1458L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [avg(IncomePerCapita_zip#1344)]\n",
      "Aggregate Attributes [1]: [avg(IncomePerCapita_zip#1344)#1407]\n",
      "Results [2]: [COMM#110, round(avg(IncomePerCapita_zip#1344)#1407, 3) AS IncomePerCapita#1408]\n",
      "\n",
      "(18) Sort\n",
      "Input [2]: [COMM#110, IncomePerCapita#1408]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#361, LON#362]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "Condition : ((isnotnull(LAT#361) AND isnotnull(LON#362)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#475]\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [LAT#417, LON#418]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(23) Filter\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "Condition : ((isnotnull(LAT#417) AND isnotnull(LON#418)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1517]\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "\n",
      "(25) Union\n",
      "\n",
      "(26) Scan geojson \n",
      "Output [1]: [features#1412]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(27) Filter\n",
      "Input [1]: [features#1412]\n",
      "Condition : ((size(features#1412, true) > 0) AND isnotnull(features#1412))\n",
      "\n",
      "(28) Generate\n",
      "Input [1]: [features#1412]\n",
      "Arguments: explode(features#1412), false, [features#94]\n",
      "\n",
      "(29) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(30) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.geometry AS geometry#97]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(31) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, geometry#97]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#300]\n",
      "Results [2]: [COMM#110, buf#301]\n",
      "\n",
      "(32) Exchange\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=6298]\n",
      "\n",
      "(33) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#97)#260]\n",
      "Results [2]: [COMM#110, ST_Union_Aggr(geometry#97)#260 AS geometry#261]\n",
      "\n",
      "(34) Filter\n",
      "Input [2]: [COMM#110, geometry#261]\n",
      "Condition : isnotnull(geometry#261)\n",
      "\n",
      "(35) RangeJoin\n",
      "Arguments: geometry#475: geometry, geometry#261: geometry, WITHIN\n",
      "\n",
      "(36) Project\n",
      "Output [1]: [COMM#110]\n",
      "Input [3]: [geometry#475, COMM#110, geometry#261]\n",
      "\n",
      "(37) Filter\n",
      "Input [1]: [COMM#110]\n",
      "Condition : bloomfilter#1519 of [bf1519 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "\n",
      "(38) HashAggregate\n",
      "Input [1]: [COMM#110]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#631L]\n",
      "Results [2]: [COMM#110, count#632L]\n",
      "\n",
      "(39) Exchange\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=6469]\n",
      "\n",
      "(40) HashAggregate\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#610L]\n",
      "Results [2]: [COMM#110 AS Community#573, count(1)#610L AS CrimeCount#611L]\n",
      "\n",
      "(41) Sort\n",
      "Input [2]: [Community#573, CrimeCount#611L]\n",
      "Arguments: [Community#573 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(42) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(43) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(44) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(45) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(46) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(47) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(48) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=6308]\n",
      "\n",
      "(49) HashAggregate\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum(POP_2010#119L)#266L]\n",
      "Results [2]: [COMM#110, sum(POP_2010#119L)#266L AS TotalPopulation#267L]\n",
      "\n",
      "(50) Sort\n",
      "Input [2]: [COMM#110, TotalPopulation#267L]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(51) SortMergeJoin\n",
      "Left keys [1]: [Community#573]\n",
      "Right keys [1]: [COMM#110]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(52) Project\n",
      "Output [2]: [Community#573, round((cast(CrimeCount#611L as double) / cast(TotalPopulation#267L as double)), 6) AS CrimesPerCapita#695]\n",
      "Input [4]: [Community#573, CrimeCount#611L, COMM#110, TotalPopulation#267L]\n",
      "\n",
      "(53) Filter\n",
      "Input [2]: [Community#573, CrimesPerCapita#695]\n",
      "Condition : bloomfilter#1518 of [bf1518 COMM#110 estimatedNumRows=294857] filtering [Community#573]\n",
      "\n",
      "(54) SortMergeJoin\n",
      "Left keys [1]: [COMM#110]\n",
      "Right keys [1]: [Community#573]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(55) Project\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1408]\n",
      "Input [4]: [COMM#110, IncomePerCapita#1408, Community#573, CrimesPerCapita#695]\n",
      "\n",
      "(56) Exchange\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1408]\n",
      "Arguments: rangepartitioning(IncomePerCapita#1408 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=6487]\n",
      "\n",
      "(57) Sort\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1408]\n",
      "Arguments: [IncomePerCapita#1408 DESC NULLS LAST], true, 0\n",
      "\n",
      "(58) AdaptiveSparkPlan\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1408]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 53 Hosting Expression = bloomfilter#1518 of [bf1518 COMM#110 estimatedNumRows=294857] filtering [Community#573]\n",
      "OutputAdapter (66)\n",
      "+- AdaptiveSparkPlan (65)\n",
      "   +- Exchange (64)\n",
      "      +- Project (63)\n",
      "         +- Filter (62)\n",
      "            +- Generate (61)\n",
      "               +- Filter (60)\n",
      "                  +- Scan geojson  (59)\n",
      "\n",
      "\n",
      "(59) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(60) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(61) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(62) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(63) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(64) Exchange\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: hashpartitioning(ZCTA10#127, 1000), ENSURE_REQUIREMENTS, [plan_id=6480]\n",
      "\n",
      "(65) AdaptiveSparkPlan\n",
      "Output [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(66) OutputAdapter\n",
      "Output [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "\n",
      "Subquery:2 Hosting operator id = 37 Hosting Expression = bloomfilter#1519 of [bf1519 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "OutputAdapter (75)\n",
      "+- AdaptiveSparkPlan (74)\n",
      "   +- Exchange (73)\n",
      "      +- HashAggregate (72)\n",
      "         +- Project (71)\n",
      "            +- Filter (70)\n",
      "               +- Generate (69)\n",
      "                  +- Filter (68)\n",
      "                     +- Scan geojson  (67)\n",
      "\n",
      "\n",
      "(67) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(68) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(69) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(70) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(71) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(72) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(73) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=6463]\n",
      "\n",
      "(74) AdaptiveSparkPlan\n",
      "Output [2]: [COMM#110, sum#307L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(75) OutputAdapter\n",
      "Output [2]: [COMM#110, sum#307L]"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94306bc8-8054-4f66-8a20-e8a5a0492aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+---------------+\n",
      "|Community              |CrimesPerCapita|IncomePerCapita|\n",
      "+-----------------------+---------------+---------------+\n",
      "|Beverly Crest          |0.368961       |80637.577      |\n",
      "|Pacific Palisades      |0.379676       |71315.178      |\n",
      "|Bel Air                |0.399225       |70879.469      |\n",
      "|Venice                 |1.040428       |67888.88       |\n",
      "|Palisades Highlands    |0.187842       |67392.081      |\n",
      "|Marina Peninsula       |0.599954       |63010.679      |\n",
      "|Mandeville Canyon      |0.261058       |62110.61       |\n",
      "|Brentwood              |0.405864       |56551.715      |\n",
      "|Playa Vista            |0.500448       |55354.295      |\n",
      "|Carthay                |0.762896       |47221.492      |\n",
      "|Playa Del Rey          |0.742559       |45486.718      |\n",
      "|Hollywood Hills        |0.747616       |41672.817      |\n",
      "|Studio City            |0.783407       |41151.145      |\n",
      "|Century City           |0.632969       |40671.613      |\n",
      "|South Carthay          |0.723174       |39566.826      |\n",
      "|West Los Angeles       |0.619004       |38641.406      |\n",
      "|Rancho Park            |1.008644       |37349.171      |\n",
      "|Angeles National Forest|0.411765       |36448.933      |\n",
      "|Encino                 |0.623781       |36441.693      |\n",
      "|Woodland Hills         |0.545094       |36285.191      |\n",
      "+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken (shuffle_hash join): 57.31 seconds"
     ]
    }
   ],
   "source": [
    "# SHUFFLE HASH JOIN\n",
    "\n",
    "from pyspark.sql.functions import explode, col, sum\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.hint(\"shuffle_hash\").join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# avg_income for each community (each community has multiple zip codes)\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "result_df = aggregated_df.hint(\"shuffle_hash\").join(\n",
    "    crime_with_population,\n",
    "    crime_with_population[\"Community\"] == aggregated_df[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimesPerCapita\"),\n",
    "    col(\"IncomePerCapita\"),\n",
    ")\n",
    "\n",
    "# Sort the results in descending order by IncomePerCapita\n",
    "result_df = result_df.orderBy(col(\"IncomePerCapita\").desc())\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken (shuffle_hash join): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1b6f081-f5ea-4707-86da-3aa813989643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (55)\n",
      "+- Sort (54)\n",
      "   +- Exchange (53)\n",
      "      +- Project (52)\n",
      "         +- ShuffledHashJoin Inner BuildLeft (51)\n",
      "            :- HashAggregate (15)\n",
      "            :  +- Exchange (14)\n",
      "            :     +- HashAggregate (13)\n",
      "            :        +- Project (12)\n",
      "            :           +- ShuffledHashJoin Inner BuildLeft (11)\n",
      "            :              :- Exchange (6)\n",
      "            :              :  +- Project (5)\n",
      "            :              :     +- Filter (4)\n",
      "            :              :        +- Generate (3)\n",
      "            :              :           +- Filter (2)\n",
      "            :              :              +- Scan geojson  (1)\n",
      "            :              +- Exchange (10)\n",
      "            :                 +- Project (9)\n",
      "            :                    +- Filter (8)\n",
      "            :                       +- Scan csv  (7)\n",
      "            +- Filter (50)\n",
      "               +- Project (49)\n",
      "                  +- SortMergeJoin Inner (48)\n",
      "                     :- Sort (38)\n",
      "                     :  +- HashAggregate (37)\n",
      "                     :     +- Exchange (36)\n",
      "                     :        +- HashAggregate (35)\n",
      "                     :           +- Filter (34)\n",
      "                     :              +- Project (33)\n",
      "                     :                 +- RangeJoin (32)\n",
      "                     :                    :- Union (22)\n",
      "                     :                    :  :- Project (18)\n",
      "                     :                    :  :  +- Filter (17)\n",
      "                     :                    :  :     +- Scan csv  (16)\n",
      "                     :                    :  +- Project (21)\n",
      "                     :                    :     +- Filter (20)\n",
      "                     :                    :        +- Scan csv  (19)\n",
      "                     :                    +- Filter (31)\n",
      "                     :                       +- ObjectHashAggregate (30)\n",
      "                     :                          +- Exchange (29)\n",
      "                     :                             +- ObjectHashAggregate (28)\n",
      "                     :                                +- Project (27)\n",
      "                     :                                   +- Filter (26)\n",
      "                     :                                      +- Generate (25)\n",
      "                     :                                         +- Filter (24)\n",
      "                     :                                            +- Scan geojson  (23)\n",
      "                     +- Sort (47)\n",
      "                        +- HashAggregate (46)\n",
      "                           +- Exchange (45)\n",
      "                              +- HashAggregate (44)\n",
      "                                 +- Project (43)\n",
      "                                    +- Filter (42)\n",
      "                                       +- Generate (41)\n",
      "                                          +- Filter (40)\n",
      "                                             +- Scan geojson  (39)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(6) Exchange\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: hashpartitioning(ZCTA10#127, 1000), ENSURE_REQUIREMENTS, [plan_id=8382]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [3]: [ZIP#0, Comm#1, income#2]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Comm), StringContains(Comm,Los Angeles), IsNotNull(ZIP)]\n",
      "ReadSchema: struct<ZIP:string,Comm:string,income:string>\n",
      "\n",
      "(8) Filter\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "Condition : ((isnotnull(Comm#1) AND Contains(Comm#1, Los Angeles)) AND isnotnull(ZIP#0))\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [ZIP#0 AS ZIP_Income#760, cast(regexp_replace(income#2, [$,], , 1) as double) AS income_cleaned#750]\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "\n",
      "(10) Exchange\n",
      "Input [2]: [ZIP_Income#760, income_cleaned#750]\n",
      "Arguments: hashpartitioning(ZIP_Income#760, 1000), ENSURE_REQUIREMENTS, [plan_id=8383]\n",
      "\n",
      "(11) ShuffledHashJoin\n",
      "Left keys [1]: [ZCTA10#127]\n",
      "Right keys [1]: [ZIP_Income#760]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [2]: [COMM#110, ((income_cleaned#750 * cast(HOUSING10#116L as double)) / cast(POP_2010#119L as double)) AS IncomePerCapita_zip#1596]\n",
      "Input [6]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127, ZIP_Income#760, income_cleaned#750]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [2]: [COMM#110, IncomePerCapita_zip#1596]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_avg(IncomePerCapita_zip#1596)]\n",
      "Aggregate Attributes [2]: [sum#1707, count#1708L]\n",
      "Results [3]: [COMM#110, sum#1709, count#1710L]\n",
      "\n",
      "(14) Exchange\n",
      "Input [3]: [COMM#110, sum#1709, count#1710L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=8388]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [3]: [COMM#110, sum#1709, count#1710L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [avg(IncomePerCapita_zip#1596)]\n",
      "Aggregate Attributes [1]: [avg(IncomePerCapita_zip#1596)#1659]\n",
      "Results [2]: [COMM#110, round(avg(IncomePerCapita_zip#1596)#1659, 3) AS IncomePerCapita#1660]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#361, LON#362]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "Condition : ((isnotnull(LAT#361) AND isnotnull(LON#362)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#475]\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#417, LON#418]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "Condition : ((isnotnull(LAT#417) AND isnotnull(LON#418)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1763]\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan geojson \n",
      "Output [1]: [features#1664]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#1664]\n",
      "Condition : ((size(features#1664, true) > 0) AND isnotnull(features#1664))\n",
      "\n",
      "(25) Generate\n",
      "Input [1]: [features#1664]\n",
      "Arguments: explode(features#1664), false, [features#94]\n",
      "\n",
      "(26) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.geometry AS geometry#97]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(28) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, geometry#97]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#300]\n",
      "Results [2]: [COMM#110, buf#301]\n",
      "\n",
      "(29) Exchange\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=8390]\n",
      "\n",
      "(30) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#97)#260]\n",
      "Results [2]: [COMM#110, ST_Union_Aggr(geometry#97)#260 AS geometry#261]\n",
      "\n",
      "(31) Filter\n",
      "Input [2]: [COMM#110, geometry#261]\n",
      "Condition : isnotnull(geometry#261)\n",
      "\n",
      "(32) RangeJoin\n",
      "Arguments: geometry#475: geometry, geometry#261: geometry, WITHIN\n",
      "\n",
      "(33) Project\n",
      "Output [1]: [COMM#110]\n",
      "Input [3]: [geometry#475, COMM#110, geometry#261]\n",
      "\n",
      "(34) Filter\n",
      "Input [1]: [COMM#110]\n",
      "Condition : bloomfilter#1765 of [bf1765 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [1]: [COMM#110]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#631L]\n",
      "Results [2]: [COMM#110, count#632L]\n",
      "\n",
      "(36) Exchange\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=8553]\n",
      "\n",
      "(37) HashAggregate\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#610L]\n",
      "Results [2]: [COMM#110 AS Community#573, count(1)#610L AS CrimeCount#611L]\n",
      "\n",
      "(38) Sort\n",
      "Input [2]: [Community#573, CrimeCount#611L]\n",
      "Arguments: [Community#573 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(39) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(40) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(41) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(42) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(43) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(44) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(45) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=8400]\n",
      "\n",
      "(46) HashAggregate\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum(POP_2010#119L)#266L]\n",
      "Results [2]: [COMM#110, sum(POP_2010#119L)#266L AS TotalPopulation#267L]\n",
      "\n",
      "(47) Sort\n",
      "Input [2]: [COMM#110, TotalPopulation#267L]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(48) SortMergeJoin\n",
      "Left keys [1]: [Community#573]\n",
      "Right keys [1]: [COMM#110]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(49) Project\n",
      "Output [2]: [Community#573, round((cast(CrimeCount#611L as double) / cast(TotalPopulation#267L as double)), 6) AS CrimesPerCapita#695]\n",
      "Input [4]: [Community#573, CrimeCount#611L, COMM#110, TotalPopulation#267L]\n",
      "\n",
      "(50) Filter\n",
      "Input [2]: [Community#573, CrimesPerCapita#695]\n",
      "Condition : bloomfilter#1764 of [bf1764 COMM#110 estimatedNumRows=294857] filtering [Community#573]\n",
      "\n",
      "(51) ShuffledHashJoin\n",
      "Left keys [1]: [COMM#110]\n",
      "Right keys [1]: [Community#573]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(52) Project\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1660]\n",
      "Input [4]: [COMM#110, IncomePerCapita#1660, Community#573, CrimesPerCapita#695]\n",
      "\n",
      "(53) Exchange\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1660]\n",
      "Arguments: rangepartitioning(IncomePerCapita#1660 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=8571]\n",
      "\n",
      "(54) Sort\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1660]\n",
      "Arguments: [IncomePerCapita#1660 DESC NULLS LAST], true, 0\n",
      "\n",
      "(55) AdaptiveSparkPlan\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1660]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 50 Hosting Expression = bloomfilter#1764 of [bf1764 COMM#110 estimatedNumRows=294857] filtering [Community#573]\n",
      "OutputAdapter (63)\n",
      "+- AdaptiveSparkPlan (62)\n",
      "   +- Exchange (61)\n",
      "      +- Project (60)\n",
      "         +- Filter (59)\n",
      "            +- Generate (58)\n",
      "               +- Filter (57)\n",
      "                  +- Scan geojson  (56)\n",
      "\n",
      "\n",
      "(56) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(57) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(58) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(59) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(60) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(61) Exchange\n",
      "Input [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: hashpartitioning(ZCTA10#127, 1000), ENSURE_REQUIREMENTS, [plan_id=8564]\n",
      "\n",
      "(62) AdaptiveSparkPlan\n",
      "Output [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(63) OutputAdapter\n",
      "Output [4]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127]\n",
      "\n",
      "Subquery:2 Hosting operator id = 34 Hosting Expression = bloomfilter#1765 of [bf1765 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "OutputAdapter (72)\n",
      "+- AdaptiveSparkPlan (71)\n",
      "   +- Exchange (70)\n",
      "      +- HashAggregate (69)\n",
      "         +- Project (68)\n",
      "            +- Filter (67)\n",
      "               +- Generate (66)\n",
      "                  +- Filter (65)\n",
      "                     +- Scan geojson  (64)\n",
      "\n",
      "\n",
      "(64) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(65) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(66) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(67) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(68) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(69) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(70) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=8547]\n",
      "\n",
      "(71) AdaptiveSparkPlan\n",
      "Output [2]: [COMM#110, sum#307L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(72) OutputAdapter\n",
      "Output [2]: [COMM#110, sum#307L]"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1d51cf6-b638-453a-8d6b-6e65a82b8267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+---------------+\n",
      "|Community              |CrimesPerCapita|IncomePerCapita|\n",
      "+-----------------------+---------------+---------------+\n",
      "|Beverly Crest          |0.368961       |80637.577      |\n",
      "|Pacific Palisades      |0.379676       |71315.178      |\n",
      "|Bel Air                |0.399225       |70879.469      |\n",
      "|Venice                 |1.040428       |67888.88       |\n",
      "|Palisades Highlands    |0.187842       |67392.081      |\n",
      "|Marina Peninsula       |0.599954       |63010.679      |\n",
      "|Mandeville Canyon      |0.261058       |62110.61       |\n",
      "|Brentwood              |0.405864       |56551.715      |\n",
      "|Playa Vista            |0.500448       |55354.295      |\n",
      "|Carthay                |0.762896       |47221.492      |\n",
      "|Playa Del Rey          |0.742559       |45486.718      |\n",
      "|Hollywood Hills        |0.747616       |41672.817      |\n",
      "|Studio City            |0.783407       |41151.145      |\n",
      "|Century City           |0.632969       |40671.613      |\n",
      "|South Carthay          |0.723174       |39566.826      |\n",
      "|West Los Angeles       |0.619004       |38641.406      |\n",
      "|Rancho Park            |1.008644       |37349.171      |\n",
      "|Angeles National Forest|0.411765       |36448.933      |\n",
      "|Encino                 |0.623781       |36441.693      |\n",
      "|Woodland Hills         |0.545094       |36285.191      |\n",
      "+-----------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken (shuffle_replicate_nl join): 62.73 seconds"
     ]
    }
   ],
   "source": [
    "# SHUFFLE_REPLICATE_NL JOIN\n",
    "\n",
    "from pyspark.sql.functions import explode, col, sum\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.hint(\"shuffle_replicate_nl\").join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# avg_income for each community (each community has multiple zip codes)\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "result_df = aggregated_df.hint(\"shuffle_replicate_nl\").join(\n",
    "    crime_with_population,\n",
    "    crime_with_population[\"Community\"] == aggregated_df[\"COMM\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"CrimesPerCapita\"),\n",
    "    col(\"IncomePerCapita\"),\n",
    ")\n",
    "\n",
    "# Sort the results in descending order by IncomePerCapita\n",
    "result_df = result_df.orderBy(col(\"IncomePerCapita\").desc())\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken (shuffle_replicate_nl join): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccfbae84-99a5-44bf-8d9d-76307fe93afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (53)\n",
      "+- Sort (52)\n",
      "   +- Exchange (51)\n",
      "      +- Project (50)\n",
      "         +- CartesianProduct Inner (49)\n",
      "            :- HashAggregate (13)\n",
      "            :  +- Exchange (12)\n",
      "            :     +- HashAggregate (11)\n",
      "            :        +- Project (10)\n",
      "            :           +- CartesianProduct Inner (9)\n",
      "            :              :- Project (5)\n",
      "            :              :  +- Filter (4)\n",
      "            :              :     +- Generate (3)\n",
      "            :              :        +- Filter (2)\n",
      "            :              :           +- Scan geojson  (1)\n",
      "            :              +- Project (8)\n",
      "            :                 +- Filter (7)\n",
      "            :                    +- Scan csv  (6)\n",
      "            +- Filter (48)\n",
      "               +- Project (47)\n",
      "                  +- SortMergeJoin Inner (46)\n",
      "                     :- Sort (36)\n",
      "                     :  +- HashAggregate (35)\n",
      "                     :     +- Exchange (34)\n",
      "                     :        +- HashAggregate (33)\n",
      "                     :           +- Filter (32)\n",
      "                     :              +- Project (31)\n",
      "                     :                 +- RangeJoin (30)\n",
      "                     :                    :- Union (20)\n",
      "                     :                    :  :- Project (16)\n",
      "                     :                    :  :  +- Filter (15)\n",
      "                     :                    :  :     +- Scan csv  (14)\n",
      "                     :                    :  +- Project (19)\n",
      "                     :                    :     +- Filter (18)\n",
      "                     :                    :        +- Scan csv  (17)\n",
      "                     :                    +- Filter (29)\n",
      "                     :                       +- ObjectHashAggregate (28)\n",
      "                     :                          +- Exchange (27)\n",
      "                     :                             +- ObjectHashAggregate (26)\n",
      "                     :                                +- Project (25)\n",
      "                     :                                   +- Filter (24)\n",
      "                     :                                      +- Generate (23)\n",
      "                     :                                         +- Filter (22)\n",
      "                     :                                            +- Scan geojson  (21)\n",
      "                     +- Sort (45)\n",
      "                        +- HashAggregate (44)\n",
      "                           +- Exchange (43)\n",
      "                              +- HashAggregate (42)\n",
      "                                 +- Project (41)\n",
      "                                    +- Filter (40)\n",
      "                                       +- Generate (39)\n",
      "                                          +- Filter (38)\n",
      "                                             +- Scan geojson  (37)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#86]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#86]\n",
      "Condition : ((size(features#86, true) > 0) AND isnotnull(features#86))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#86]\n",
      "Arguments: explode(features#86), false, [features#94]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND (isnotnull(features#94.properties.ZCTA10) AND isnotnull(features#94.properties.COMM)))\n",
      "\n",
      "(5) Project\n",
      "Output [4]: [features#94.properties.COMM AS COMM#110, features#94.properties.HOUSING10 AS HOUSING10#116L, features#94.properties.POP_2010 AS POP_2010#119L, features#94.properties.ZCTA10 AS ZCTA10#127]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [3]: [ZIP#0, Comm#1, income#2]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Comm), StringContains(Comm,Los Angeles), IsNotNull(ZIP)]\n",
      "ReadSchema: struct<ZIP:string,Comm:string,income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "Condition : ((isnotnull(Comm#1) AND Contains(Comm#1, Los Angeles)) AND isnotnull(ZIP#0))\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [ZIP#0 AS ZIP_Income#760, cast(regexp_replace(income#2, [$,], , 1) as double) AS income_cleaned#750]\n",
      "Input [3]: [ZIP#0, Comm#1, income#2]\n",
      "\n",
      "(9) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (ZCTA10#127 = ZIP_Income#760)\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [COMM#110, ((income_cleaned#750 * cast(HOUSING10#116L as double)) / cast(POP_2010#119L as double)) AS IncomePerCapita_zip#1842]\n",
      "Input [6]: [COMM#110, HOUSING10#116L, POP_2010#119L, ZCTA10#127, ZIP_Income#760, income_cleaned#750]\n",
      "\n",
      "(11) HashAggregate\n",
      "Input [2]: [COMM#110, IncomePerCapita_zip#1842]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_avg(IncomePerCapita_zip#1842)]\n",
      "Aggregate Attributes [2]: [sum#1953, count#1954L]\n",
      "Results [3]: [COMM#110, sum#1955, count#1956L]\n",
      "\n",
      "(12) Exchange\n",
      "Input [3]: [COMM#110, sum#1955, count#1956L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=10057]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [COMM#110, sum#1955, count#1956L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [avg(IncomePerCapita_zip#1842)]\n",
      "Aggregate Attributes [1]: [avg(IncomePerCapita_zip#1842)#1905]\n",
      "Results [2]: [COMM#110, round(avg(IncomePerCapita_zip#1842)#1905, 3) AS IncomePerCapita#1906]\n",
      "\n",
      "(14) Scan csv \n",
      "Output [2]: [LAT#361, LON#362]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(15) Filter\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "Condition : ((isnotnull(LAT#361) AND isnotnull(LON#362)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(16) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#475]\n",
      "Input [2]: [LAT#361, LON#362]\n",
      "\n",
      "(17) Scan csv \n",
      "Output [2]: [LAT#417, LON#418]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON)]\n",
      "ReadSchema: struct<LAT:string,LON:string>\n",
      "\n",
      "(18) Filter\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "Condition : ((isnotnull(LAT#417) AND isnotnull(LON#418)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "\n",
      "(19) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#2007]\n",
      "Input [2]: [LAT#417, LON#418]\n",
      "\n",
      "(20) Union\n",
      "\n",
      "(21) Scan geojson \n",
      "Output [1]: [features#1910]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(22) Filter\n",
      "Input [1]: [features#1910]\n",
      "Condition : ((size(features#1910, true) > 0) AND isnotnull(features#1910))\n",
      "\n",
      "(23) Generate\n",
      "Input [1]: [features#1910]\n",
      "Arguments: explode(features#1910), false, [features#94]\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(25) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.geometry AS geometry#97]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(26) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, geometry#97]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [buf#300]\n",
      "Results [2]: [COMM#110, buf#301]\n",
      "\n",
      "(27) Exchange\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=10059]\n",
      "\n",
      "(28) ObjectHashAggregate\n",
      "Input [2]: [COMM#110, buf#301]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [st_union_aggr(geometry#97, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2a081577, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [1]: [ST_Union_Aggr(geometry#97)#260]\n",
      "Results [2]: [COMM#110, ST_Union_Aggr(geometry#97)#260 AS geometry#261]\n",
      "\n",
      "(29) Filter\n",
      "Input [2]: [COMM#110, geometry#261]\n",
      "Condition : isnotnull(geometry#261)\n",
      "\n",
      "(30) RangeJoin\n",
      "Arguments: geometry#475: geometry, geometry#261: geometry, WITHIN\n",
      "\n",
      "(31) Project\n",
      "Output [1]: [COMM#110]\n",
      "Input [3]: [geometry#475, COMM#110, geometry#261]\n",
      "\n",
      "(32) Filter\n",
      "Input [1]: [COMM#110]\n",
      "Condition : bloomfilter#2009 of [bf2009 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "\n",
      "(33) HashAggregate\n",
      "Input [1]: [COMM#110]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#631L]\n",
      "Results [2]: [COMM#110, count#632L]\n",
      "\n",
      "(34) Exchange\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=10214]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [2]: [COMM#110, count#632L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#610L]\n",
      "Results [2]: [COMM#110 AS Community#573, count(1)#610L AS CrimeCount#611L]\n",
      "\n",
      "(36) Sort\n",
      "Input [2]: [Community#573, CrimeCount#611L]\n",
      "Arguments: [Community#573 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(37) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(38) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(39) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(40) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(41) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(42) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(43) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=10069]\n",
      "\n",
      "(44) HashAggregate\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum(POP_2010#119L)#266L]\n",
      "Results [2]: [COMM#110, sum(POP_2010#119L)#266L AS TotalPopulation#267L]\n",
      "\n",
      "(45) Sort\n",
      "Input [2]: [COMM#110, TotalPopulation#267L]\n",
      "Arguments: [COMM#110 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(46) SortMergeJoin\n",
      "Left keys [1]: [Community#573]\n",
      "Right keys [1]: [COMM#110]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(47) Project\n",
      "Output [2]: [Community#573, round((cast(CrimeCount#611L as double) / cast(TotalPopulation#267L as double)), 6) AS CrimesPerCapita#695]\n",
      "Input [4]: [Community#573, CrimeCount#611L, COMM#110, TotalPopulation#267L]\n",
      "\n",
      "(48) Filter\n",
      "Input [2]: [Community#573, CrimesPerCapita#695]\n",
      "Condition : true\n",
      "\n",
      "(49) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (Community#573 = COMM#110)\n",
      "\n",
      "(50) Project\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1906]\n",
      "Input [4]: [COMM#110, IncomePerCapita#1906, Community#573, CrimesPerCapita#695]\n",
      "\n",
      "(51) Exchange\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1906]\n",
      "Arguments: rangepartitioning(IncomePerCapita#1906 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=10223]\n",
      "\n",
      "(52) Sort\n",
      "Input [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1906]\n",
      "Arguments: [IncomePerCapita#1906 DESC NULLS LAST], true, 0\n",
      "\n",
      "(53) AdaptiveSparkPlan\n",
      "Output [3]: [Community#573, CrimesPerCapita#695, IncomePerCapita#1906]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 32 Hosting Expression = bloomfilter#2009 of [bf2009 COMM#110 estimatedNumRows=294857] filtering [COMM#110]\n",
      "OutputAdapter (62)\n",
      "+- AdaptiveSparkPlan (61)\n",
      "   +- Exchange (60)\n",
      "      +- HashAggregate (59)\n",
      "         +- Project (58)\n",
      "            +- Filter (57)\n",
      "               +- Generate (56)\n",
      "                  +- Filter (55)\n",
      "                     +- Scan geojson  (54)\n",
      "\n",
      "\n",
      "(54) Scan geojson \n",
      "Output [1]: [features#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(55) Filter\n",
      "Input [1]: [features#660]\n",
      "Condition : ((size(features#660, true) > 0) AND isnotnull(features#660))\n",
      "\n",
      "(56) Generate\n",
      "Input [1]: [features#660]\n",
      "Arguments: explode(features#660), false, [features#94]\n",
      "\n",
      "(57) Filter\n",
      "Input [1]: [features#94]\n",
      "Condition : ((isnotnull(features#94.properties.CITY) AND (((((features#94.properties.CITY = Los Angeles) AND isnotnull(features#94.properties.HOUSING10)) AND (features#94.properties.HOUSING10 > 0)) AND isnotnull(features#94.properties.POP_2010)) AND (features#94.properties.POP_2010 > 0))) AND isnotnull(features#94.properties.COMM))\n",
      "\n",
      "(58) Project\n",
      "Output [2]: [features#94.properties.COMM AS COMM#110, features#94.properties.POP_2010 AS POP_2010#119L]\n",
      "Input [1]: [features#94]\n",
      "\n",
      "(59) HashAggregate\n",
      "Input [2]: [COMM#110, POP_2010#119L]\n",
      "Keys [1]: [COMM#110]\n",
      "Functions [1]: [partial_sum(POP_2010#119L)]\n",
      "Aggregate Attributes [1]: [sum#306L]\n",
      "Results [2]: [COMM#110, sum#307L]\n",
      "\n",
      "(60) Exchange\n",
      "Input [2]: [COMM#110, sum#307L]\n",
      "Arguments: hashpartitioning(COMM#110, 1000), ENSURE_REQUIREMENTS, [plan_id=10208]\n",
      "\n",
      "(61) AdaptiveSparkPlan\n",
      "Output [2]: [COMM#110, sum#307L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(62) OutputAdapter\n",
      "Output [2]: [COMM#110, sum#307L]"
     ]
    }
   ],
   "source": [
    "result_df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5298a1-56f7-454f-9b47-43e96908a16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
