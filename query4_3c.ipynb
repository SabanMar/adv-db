{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38aca1e4-3b3f-4565-93c0-c21ecf8ba87b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4', 'spark.driver.memory': '8g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3283</td><td>application_1732639283265_3239</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3239/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3239_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3292</td><td>application_1732639283265_3248</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3248/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3248_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3297</td><td>application_1732639283265_3253</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3253/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3253_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3307</td><td>application_1732639283265_3263</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3263/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3263_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3349</td><td>application_1732639283265_3305</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3305/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3305_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3399</td><td>application_1732639283265_3355</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3355/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3355_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3417</td><td>application_1732639283265_3373</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3373/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3373_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3419</td><td>application_1732639283265_3375</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3375/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3375_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3420</td><td>application_1732639283265_3376</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3376/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-233.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3376_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3421</td><td>application_1732639283265_3377</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3377/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3377_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3423</td><td>application_1732639283265_3379</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3379/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3379_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3426</td><td>application_1732639283265_3382</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3382/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3382_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3429</td><td>application_1732639283265_3385</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3385/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3385_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3430</td><td>application_1732639283265_3386</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3386/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-178.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3386_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3436</td><td>application_1732639283265_3392</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3392/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3392_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3440</td><td>application_1732639283265_3396</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3396/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3396_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3442</td><td>application_1732639283265_3398</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3398/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-227.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3398_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3444</td><td>application_1732639283265_3400</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3400/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3400_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3446</td><td>application_1732639283265_3402</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3402/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-16.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3402_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3450</td><td>application_1732639283265_3406</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3406/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3406_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3452</td><td>application_1732639283265_3408</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3408/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3408_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3453</td><td>application_1732639283265_3409</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3409/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3409_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3454</td><td>application_1732639283265_3410</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3410/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3410_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3460</td><td>application_1732639283265_3416</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3416/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3416_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3461</td><td>application_1732639283265_3417</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3417/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-94.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3417_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>3462</td><td>application_1732639283265_3418</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3418/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3418_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.driver.memory\": \"8g\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be9467b-004f-45af-9d56-8341c15cde31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3464</td><td>application_1732639283265_3420</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3420/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3420_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racial Profile of Crime Victims in Top 3 Areas by Income:\n",
      "+-----------------+------------------------------+-----+\n",
      "|Community        |Victim_Ethnicity              |count|\n",
      "+-----------------+------------------------------+-----+\n",
      "|Bel Air          |White                         |148  |\n",
      "|Bel Air          |Other                         |56   |\n",
      "|Bel Air          |Hispanic/Latin/Mexican        |15   |\n",
      "|Bel Air          |Black                         |7    |\n",
      "|Bel Air          |Unknown                       |6    |\n",
      "|Bel Air          |Other Asian                   |5    |\n",
      "|Beverly Crest    |White                         |252  |\n",
      "|Beverly Crest    |Other                         |77   |\n",
      "|Beverly Crest    |Hispanic/Latin/Mexican        |23   |\n",
      "|Beverly Crest    |Black                         |13   |\n",
      "|Beverly Crest    |Other Asian                   |12   |\n",
      "|Beverly Crest    |Unknown                       |8    |\n",
      "|Pacific Palisades|White                         |471  |\n",
      "|Pacific Palisades|Other                         |49   |\n",
      "|Pacific Palisades|Hispanic/Latin/Mexican        |44   |\n",
      "|Pacific Palisades|Unknown                       |29   |\n",
      "|Pacific Palisades|Black                         |26   |\n",
      "|Pacific Palisades|Other Asian                   |12   |\n",
      "|Pacific Palisades|Chinese                       |1    |\n",
      "|Pacific Palisades|American Indian/Alaskan Native|1    |\n",
      "+-----------------+------------------------------+-----+\n",
      "\n",
      "Racial Profile of Crime Victims in Bottom 3 Areas by Income:\n",
      "+---------------+------------------------------+-----+\n",
      "|Community      |Victim_Ethnicity              |count|\n",
      "+---------------+------------------------------+-----+\n",
      "|Central        |Hispanic/Latin/Mexican        |1060 |\n",
      "|Central        |Black                         |211  |\n",
      "|Central        |Other                         |65   |\n",
      "|Central        |White                         |32   |\n",
      "|Central        |Other Asian                   |4    |\n",
      "|Central        |American Indian/Alaskan Native|1    |\n",
      "|Central        |Korean                        |1    |\n",
      "|University Park|Hispanic/Latin/Mexican        |402  |\n",
      "|University Park|White                         |277  |\n",
      "|University Park|Black                         |123  |\n",
      "|University Park|Other Asian                   |107  |\n",
      "|University Park|Other                         |76   |\n",
      "|University Park|American Indian/Alaskan Native|20   |\n",
      "|University Park|Unknown                       |19   |\n",
      "|University Park|Korean                        |4    |\n",
      "|University Park|Chinese                       |3    |\n",
      "|University Park|Asian Indian                  |1    |\n",
      "|Vernon Central |Hispanic/Latin/Mexican        |1353 |\n",
      "|Vernon Central |Black                         |427  |\n",
      "|Vernon Central |Other                         |46   |\n",
      "|Vernon Central |White                         |21   |\n",
      "|Vernon Central |Unknown                       |3    |\n",
      "|Vernon Central |Other Asian                   |2    |\n",
      "|Vernon Central |Filipino                      |1    |\n",
      "+---------------+------------------------------+-----+\n",
      "\n",
      "Time taken: 63.46 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from sedona.spark import *\n",
    "from sedona.sql import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"query4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"ZIP\", StringType(), True),\n",
    "    StructField(\"Comm\", StringType(), True),\n",
    "    StructField(\"income\", StringType(), True),\n",
    "])\n",
    "\n",
    "income_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .schema(income_schema) \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Define crime data schema\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1-2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict_Age\", StringType(), True),\n",
    "    StructField(\"Vict_Sex\", StringType(), True),\n",
    "    StructField(\"Vict_Descent\", StringType(), True),\n",
    "    StructField(\"Premis_Cd\", StringType(), True),\n",
    "    StructField(\"Premis_Desc\", StringType(), True),\n",
    "    StructField(\"Weapon_Used_Cd\", StringType(), True),\n",
    "    StructField(\"Weapon_Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status_Desc\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_1\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_3\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_4\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross_Street\", StringType(), True),\n",
    "    StructField(\"LAT\", StringType(), True),\n",
    "    StructField(\"LON\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read crime data\n",
    "crimes_df1 = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crimes_df2 = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "# Combine datasets\n",
    "crimes_df = crimes_df1.union(crimes_df2)\n",
    "\n",
    "# Filter valid coordinates and create geometry column for crimes\n",
    "crimes_df = crimes_df.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()))\n",
    "crimes_df = crimes_df.withColumn(\"geometry\", ST_Point(col(\"LON\").cast(\"double\"), col(\"LAT\").cast(\"double\")))\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "\n",
    "la_df = flattened_df.filter(\n",
    "    (col(\"CITY\") == \"Los Angeles\") &\n",
    "    (col(\"HOUSING10\").isNotNull()) & (col(\"HOUSING10\") > 0) &\n",
    "    (col(\"POP_2010\").isNotNull()) & (col(\"POP_2010\") > 0)\n",
    ")\n",
    "\n",
    "\n",
    "LA_areas = la_df \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        ST_Union_Aggr(\"geometry\").alias(\"geometry\"),   # Aggregate geometries\n",
    "        collect_set(\"ZCTA10\").alias(\"ZIPCodes\"),       # Collect unique ZIP codes as a list for each area\n",
    "        sum(\"HOUSING10\").alias(\"TotalHousing\"),        # Sum of HOUSING10 for each community\n",
    "        sum(\"POP_2010\").alias(\"TotalPopulation\")       # Sum of POP_2010 for each community\n",
    "    )\n",
    "\n",
    "# Clean and prepare the income column in income_df\n",
    "income_df_cleaned = income_df.withColumn(\n",
    "    \"income_cleaned\",\n",
    "    regexp_replace(col(\"income\"), \"[$,]\", \"\").cast(\"double\")  # Remove dollar signs and commas, cast to double\n",
    ")\n",
    "income_df_LA = income_df_cleaned.filter(col(\"Comm\").contains(\"Los Angeles\"))\n",
    "# Rename the columns to avoid ambiguity\n",
    "income_df_LA = income_df_LA.withColumnRenamed(\"Comm\", \"Community\")\n",
    "income_df_LA = income_df_LA.withColumnRenamed(\"ZIP\", \"ZIP_Income\")\n",
    "\n",
    "# Join the exploded DataFrame with the income DataFrame on ZipCodes, since comm doesm't match\n",
    "joined_df = la_df.join(\n",
    "    income_df_LA,\n",
    "    la_df[\"ZCTA10\"] == income_df_LA[\"ZIP_Income\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add the new column : avg_income for each zip\n",
    "joined_df_zip = joined_df.withColumn(\n",
    "    \"IncomePerCapita_zip\",\n",
    "     col(\"income_cleaned\") * col(\"HOUSING10\") / col(\"POP_2010\")\n",
    ")\n",
    "\n",
    "# avg_income for each community (each community has multiple zip codes)\n",
    "aggregated_df = joined_df_zip.groupBy(\"COMM\").agg(\n",
    "    round(avg(\"IncomePerCapita_zip\") ,3).alias(\"IncomePerCapita\")\n",
    ")\n",
    "\n",
    "# Correct datetime format for parsing\n",
    "datetime_format = \"MM/dd/yyyy hh:mm:ss a\"\n",
    "\n",
    "# Step 1: Convert DATE_OCC to a proper date format and filter for 2015\n",
    "crimes_2015 = crimes_df.withColumn(\n",
    "    \"Year_OCC\",\n",
    "    year(to_date(col(\"DATE_OCC\"), datetime_format))\n",
    ").filter(col(\"Year_OCC\") == 2015)\n",
    "\n",
    "# Step 2: Join crimes with LA areas based on geometry\n",
    "crimes_in_areas = crimes_2015.alias(\"crimes\").join(\n",
    "    LA_areas.alias(\"areas\"),\n",
    "    ST_Contains(col(\"areas.geometry\"), col(\"crimes.geometry\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"areas.COMM\").alias(\"Community\"),\n",
    "    col(\"crimes.Vict_Descent\")\n",
    ")\n",
    "\n",
    "# Step 3: Filter crimes for top 3 and bottom 3 income areas\n",
    "crimes_top3 = crimes_in_areas.join(\n",
    "    aggregated_df.orderBy(desc(\"IncomePerCapita\")).limit(3).withColumnRenamed(\"COMM\", \"HighIncomeCOMM\"),\n",
    "    crimes_in_areas[\"Community\"] == col(\"HighIncomeCOMM\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crimes_in_areas[\"Community\"],\n",
    "    crimes_in_areas[\"Vict_Descent\"]\n",
    ")\n",
    "\n",
    "crimes_bottom3 = crimes_in_areas.join(\n",
    "    aggregated_df.orderBy(asc(\"IncomePerCapita\")).limit(3).withColumnRenamed(\"COMM\", \"LowIncomeCOMM\"),\n",
    "    crimes_in_areas[\"Community\"] == col(\"LowIncomeCOMM\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    crimes_in_areas[\"Community\"],\n",
    "    crimes_in_areas[\"Vict_Descent\"]\n",
    ")\n",
    "\n",
    "# Step 4: Mapping for Vict_Descent codes to descriptions\n",
    "vict_descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "# Convert mapping dictionary to a DataFrame\n",
    "mapping_schema = StructType([\n",
    "    StructField(\"Code\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "mapping_df = spark.createDataFrame(\n",
    "    [(code, desc) for code, desc in vict_descent_mapping.items()],\n",
    "    schema=mapping_schema\n",
    ")\n",
    "\n",
    "# Step 5: Join top 3 and bottom 3 crimes with mapping\n",
    "crimes_top3_with_description = crimes_top3.join(\n",
    "    mapping_df,\n",
    "    crimes_top3[\"Vict_Descent\"] == mapping_df[\"Code\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"Description\").alias(\"Victim_Ethnicity\")\n",
    ")\n",
    "\n",
    "crimes_bottom3_with_description = crimes_bottom3.join(\n",
    "    mapping_df,\n",
    "    crimes_bottom3[\"Vict_Descent\"] == mapping_df[\"Code\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"Community\"),\n",
    "    col(\"Description\").alias(\"Victim_Ethnicity\")\n",
    ")\n",
    "\n",
    "# Remove null values from top 3 and bottom 3 results\n",
    "crimes_top3_with_description = crimes_top3_with_description.filter(col(\"Victim_Ethnicity\").isNotNull())\n",
    "crimes_bottom3_with_description = crimes_bottom3_with_description.filter(col(\"Victim_Ethnicity\").isNotNull())\n",
    "\n",
    "# Step 6: Count victims by ethnicity for top 3 areas\n",
    "race_profile_top3 = crimes_top3_with_description.groupBy(\n",
    "    \"Community\", \"Victim_Ethnicity\"\n",
    ").count().orderBy(col(\"Community\"), col(\"count\").desc())\n",
    "\n",
    "# Step 7: Count victims by ethnicity for bottom 3 areas\n",
    "race_profile_bottom3 = crimes_bottom3_with_description.groupBy(\n",
    "    \"Community\", \"Victim_Ethnicity\"\n",
    ").count().orderBy(col(\"Community\"), col(\"count\").desc())\n",
    "\n",
    "# Display all results for the top 3 areas by income\n",
    "print(\"Racial Profile of Crime Victims in Top 3 Areas by Income:\")\n",
    "race_profile_top3.show(n=100, truncate=False)\n",
    "\n",
    "# Display all results for the bottom 3 areas by income\n",
    "print(\"Racial Profile of Crime Victims in Bottom 3 Areas by Income:\")\n",
    "race_profile_bottom3.show(n=100, truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bc735-5d5d-4e2b-912b-ed907106e2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
