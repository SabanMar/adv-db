{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d458cd2a-0b6a-420a-b5a6-6c04c8abef65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1935</td><td>application_1732639283265_1896</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1896/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1896_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '1', 'spark.executor.memory': '1g', 'spark.executor.cores': '1', 'spark.driver.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1924</td><td>application_1732639283265_1885</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1885/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1885_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1925</td><td>application_1732639283265_1886</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1886/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1886_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1926</td><td>application_1732639283265_1887</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1887/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1887_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1928</td><td>application_1732639283265_1889</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1889/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1889_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1930</td><td>application_1732639283265_1891</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1891/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1891_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1931</td><td>application_1732639283265_1892</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1892/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1892_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1932</td><td>application_1732639283265_1893</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1893/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-203.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1893_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1934</td><td>application_1732639283265_1895</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1895/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1895_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1935</td><td>application_1732639283265_1896</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1896/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1896_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"1\",\n",
    "        \"spark.executor.memory\": \"1g\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.driver.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b8c9cbb-d883-47a2-9667-f8f0eced9fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------------+----+\n",
      "|Year|       Area|       Percentage|Rank|\n",
      "+----+-----------+-----------------+----+\n",
      "|2010|    Rampart|32.94735585531813|   1|\n",
      "|2010|    Olympic|31.96270619172842|   2|\n",
      "|2010|     Harbor|29.63203463203463|   3|\n",
      "|2011|    Olympic|35.21216768916155|   1|\n",
      "|2011|    Rampart|32.51177963030083|   2|\n",
      "|2011|     Harbor|28.65220520201501|   3|\n",
      "|2012|    Olympic|34.41481831052383|   1|\n",
      "|2012|    Rampart|32.94641810294290|   2|\n",
      "|2012|     Harbor|29.81513327601032|   3|\n",
      "|2013|    Olympic|33.52812271731191|   1|\n",
      "|2013|    Rampart|32.08287360549222|   2|\n",
      "|2013|     Harbor|29.16422459266206|   3|\n",
      "|2014|   Van Nuys|31.80567315834039|   1|\n",
      "|2014|West Valley|31.31198995605775|   2|\n",
      "|2014|    Mission|31.16279069767442|   3|\n",
      "|2015|   Van Nuys|32.64134698172773|   1|\n",
      "|2015|West Valley|30.27597402597403|   2|\n",
      "|2015|    Mission|30.17946067838016|   3|\n",
      "|2016|   Van Nuys|31.88075572011773|   1|\n",
      "|2016|West Valley|31.54798761609907|   2|\n",
      "+----+-----------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 29.30 seconds"
     ]
    }
   ],
   "source": [
    "# Spark SQL code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "# To log our application's execution time:\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"query2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "police_schema = StructType([\n",
    "    StructField(\"X\", FloatType(), True),\n",
    "    StructField(\"Y\", FloatType(), True),\n",
    "    StructField(\"FID\", IntegerType(), True),\n",
    "    StructField(\"DIVISION\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"PREC\", IntegerType(), True)\n",
    "])\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "police_station_df = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .schema(police_schema) \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\")\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1-2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict_Age\", StringType(), True),\n",
    "    StructField(\"Vict_Sex\", StringType(), True),\n",
    "    StructField(\"Vict_Descent\", StringType(), True),\n",
    "    StructField(\"Premis_Cd\", StringType(), True),\n",
    "    StructField(\"Premis_Desc\", StringType(), True),\n",
    "    StructField(\"Weapon_Used_Cd\", StringType(), True),\n",
    "    StructField(\"Weapon_Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status_Desc\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_1\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_3\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_4\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross_Street\", StringType(), True),\n",
    "    StructField(\"LAT\", StringType(), True),\n",
    "    StructField(\"LON\", StringType(), True)\n",
    "])\n",
    "\n",
    "crimes_df1 = spark.read.format('csv') \\\n",
    "                .options(header='false') \\\n",
    "                .schema(crimes_schema) \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crimes_df2 = spark.read.format('csv') \\\n",
    "                .options(header='true') \\\n",
    "                .schema(crimes_schema) \\\n",
    "                .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "crimes_df = crimes_df1.union(crimes_df2)\n",
    "\n",
    "# To utilize as SQL tables (replace dataframes to tables for executing sql queries)\n",
    "police_station_df.createOrReplaceTempView(\"police_station\")\n",
    "crimes_df.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "crimes_query = \"SELECT YEAR(TO_TIMESTAMP(Date_Rptd, 'MM/dd/yyyy hh:mm:ss a')) AS Year, \\\n",
    "    AREA_NAME, Status_Desc \\\n",
    "    FROM crimes \"\n",
    "\n",
    "crime_data = spark.sql(crimes_query)\n",
    "\n",
    "# Register `crime_data` as a temporary view\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "\n",
    "# SQL query to calculate the total counts and 'IC' counts for each FID\n",
    "percentage_query = \"\"\"\n",
    "    SELECT \n",
    "        c.Year, \n",
    "        c.AREA_NAME AS Area,\n",
    "        COUNT(CASE \n",
    "            WHEN c.Status_Desc IN ('Adult Arrest', 'Adult Other', 'Juv Arrest', 'Juv Other') \n",
    "            THEN 1 END) * 100.0 / COUNT(*) AS Percentage\n",
    "    FROM \n",
    "        police_station p\n",
    "    INNER JOIN \n",
    "        crime_data c\n",
    "    ON \n",
    "        UPPER(c.AREA_NAME) = p.DIVISION\n",
    "    GROUP BY \n",
    "        c.Year, c.AREA_NAME\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "percentage_data = spark.sql(percentage_query)\n",
    "\n",
    "# Define a window partitioned by `Year` and ordered by `Percentage` descending\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(col(\"Percentage\").desc())\n",
    "\n",
    "# Add a rank column to the DataFrame\n",
    "ranked_data = percentage_data.withColumn(\"Rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter the top 3 rows for each year\n",
    "top_3_per_year = ranked_data.filter(col(\"Rank\") <= 3)\n",
    "\n",
    "# Sort by Year and Rank\n",
    "final_result = top_3_per_year.orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "# Show the result\n",
    "final_result.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "021df6ff-2de5-4bcd-a1ed-02432b0d26b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|Year|       Area|        Percentage|Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart|32.947355855318136|   1|\n",
      "|2010|    Olympic|31.962706191728426|   2|\n",
      "|2010|     Harbor| 29.63203463203463|   3|\n",
      "|2011|    Olympic|35.212167689161554|   1|\n",
      "|2011|    Rampart|32.511779630300836|   2|\n",
      "|2011|     Harbor| 28.65220520201501|   3|\n",
      "|2012|    Olympic|34.414818310523835|   1|\n",
      "|2012|    Rampart|  32.9464181029429|   2|\n",
      "|2012|     Harbor|29.815133276010318|   3|\n",
      "|2013|    Olympic| 33.52812271731191|   1|\n",
      "|2013|    Rampart| 32.08287360549222|   2|\n",
      "|2013|     Harbor|29.164224592662055|   3|\n",
      "|2014|   Van Nuys| 31.80567315834039|   1|\n",
      "|2014|West Valley|31.311989956057754|   2|\n",
      "|2014|    Mission|31.162790697674417|   3|\n",
      "|2015|   Van Nuys|32.641346981727736|   1|\n",
      "|2015|West Valley|30.275974025974026|   2|\n",
      "|2015|    Mission|30.179460678380156|   3|\n",
      "|2016|   Van Nuys|31.880755720117726|   1|\n",
      "|2016|West Valley| 31.54798761609907|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 7.57 seconds"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, year, to_timestamp, when, count, row_number, upper\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"query2_dataframe\").getOrCreate()\n",
    "\n",
    "# Load police station data\n",
    "police_schema = StructType([\n",
    "    StructField(\"X\", FloatType(), True),\n",
    "    StructField(\"Y\", FloatType(), True),\n",
    "    StructField(\"FID\", IntegerType(), True),\n",
    "    StructField(\"DIVISION\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"PREC\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Load crimes data\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1-2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict_Age\", StringType(), True),\n",
    "    StructField(\"Vict_Sex\", StringType(), True),\n",
    "    StructField(\"Vict_Descent\", StringType(), True),\n",
    "    StructField(\"Premis_Cd\", StringType(), True),\n",
    "    StructField(\"Premis_Desc\", StringType(), True),\n",
    "    StructField(\"Weapon_Used_Cd\", StringType(), True),\n",
    "    StructField(\"Weapon_Desc\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Status_Desc\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_1\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_3\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_4\", StringType(), True),\n",
    "    StructField(\"LOCATION\", StringType(), True),\n",
    "    StructField(\"Cross_Street\", StringType(), True),\n",
    "    StructField(\"LAT\", StringType(), True),\n",
    "    StructField(\"LON\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Start timimg\n",
    "start_time = time.time()\n",
    "\n",
    "police_station_df = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(police_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\")\n",
    "\n",
    "crimes_df1 = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crimes_df2 = spark.read.format('csv') \\\n",
    "    .options(header='true') \\\n",
    "    .schema(crimes_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "crimes_df = crimes_df1.union(crimes_df2)\n",
    "\n",
    "\n",
    "# Extract required fields and add Year column\n",
    "crime_data_df = crimes_df \\\n",
    "    .withColumn(\"Year\", year(to_timestamp(col(\"Date_Rptd\"), \"MM/dd/yyyy hh:mm:ss a\"))) \\\n",
    "    .select(\"Year\", \"AREA_NAME\", \"Status_Desc\")\n",
    "\n",
    "# Join with police station data\n",
    "joined_data_df = crime_data_df \\\n",
    "    .join(police_station_df, upper(col(\"AREA_NAME\").cast(\"string\")) == col(\"DIVISION\").cast(\"string\"), \"inner\") \\\n",
    "    .select(\"Year\", \"Status_Desc\", col(\"AREA_NAME\").alias(\"Area\"), \"FID\")\n",
    "\n",
    "# Calculate percentages\n",
    "percentage_df = joined_data_df \\\n",
    "    .groupBy(\"Year\", \"Area\") \\\n",
    "    .agg(\n",
    "        (count(when((col(\"Status_Desc\") == \"Adult Arrest\") |\n",
    "                    (col(\"Status_Desc\") == \"Adult Other\") |\n",
    "                    (col(\"Status_Desc\") == \"Juv Arrest\") |\n",
    "                    (col(\"Status_Desc\") == \"Juv Other\"), 1)) * 100.0 / count(\"*\")).alias(\"Percentage\")\n",
    "    )\n",
    "\n",
    "# Rank areas by percentage for each year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(col(\"Percentage\").desc())\n",
    "ranked_data_df = percentage_df \\\n",
    "    .withColumn(\"Rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"Rank\") <= 3) \\\n",
    "    .orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "# Show results\n",
    "ranked_data_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31f68ecd-bbc2-4c6c-9ece-680eb025b891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|Year|       Area|        Percentage|Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart|32.947355855318136|   1|\n",
      "|2010|    Olympic|31.962706191728426|   2|\n",
      "|2010|     Harbor| 29.63203463203463|   3|\n",
      "|2011|    Olympic|35.212167689161554|   1|\n",
      "|2011|    Rampart|32.511779630300836|   2|\n",
      "|2011|     Harbor| 28.65220520201501|   3|\n",
      "|2012|    Olympic|34.414818310523835|   1|\n",
      "|2012|    Rampart|  32.9464181029429|   2|\n",
      "|2012|     Harbor|29.815133276010318|   3|\n",
      "|2013|    Olympic| 33.52812271731191|   1|\n",
      "|2013|    Rampart| 32.08287360549222|   2|\n",
      "|2013|     Harbor|29.164224592662055|   3|\n",
      "|2014|   Van Nuys| 31.80567315834039|   1|\n",
      "|2014|West Valley|31.311989956057754|   2|\n",
      "|2014|    Mission|31.162790697674417|   3|\n",
      "|2015|   Van Nuys|32.641346981727736|   1|\n",
      "|2015|West Valley|30.275974025974026|   2|\n",
      "|2015|    Mission|30.179460678380156|   3|\n",
      "|2016|   Van Nuys|31.880755720117726|   1|\n",
      "|2016|West Valley| 31.54798761609907|   2|\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Write results to S3 -> \n",
    "#    1. create the output directory in your S3 bucket\n",
    "#    2. change your group number below \n",
    "#    3. and uncomment\n",
    "group_number = \"13\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/query2_results/\"\n",
    "ranked_data_df.write.mode(\"overwrite\").parquet(s3_path)\n",
    "ranked_data_df_again = spark.read.parquet(s3_path)\n",
    "ranked_data_df_again.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
